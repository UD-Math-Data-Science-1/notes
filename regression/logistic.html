
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.6. Logistic regression &#8212; Data Science 1</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=84ace793992934648b4de8eed757e5a2" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.9d8b4a8b9bb19db25eeaddc40d639ba2.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"float": ["\\mathbb{F}"], "real": ["\\mathbb{R}"], "complex": ["\\mathbb{C}"], "nat": ["\\mathbb{N}"], "integer": ["\\mathbb{Z}"], "bfa": "\\mathbf{a}", "bfe": "\\mathbf{e}", "bfx": "\\mathbf{x}", "bfX": "\\mathbf{X}", "bfA": "\\mathbf{A}", "bfW": "\\mathbf{W}", "bfp": "\\mathbf{p}", "bfu": "\\mathbf{u}", "bfv": "\\mathbf{v}", "bfw": "\\mathbf{w}", "bfy": "\\mathbf{y}", "bfz": "\\mathbf{z}", "bfzero": "\\boldsymbol{0}", "bfmu": "\\boldsymbol{\\mu}", "TP": "\\text{TP}", "TN": "\\text{TN}", "FP": "\\text{FP}", "FN": "\\text{FN}", "rmn": ["\\mathbb{R}^{#1 \\times #2}", 2], "dd": ["\\frac{d #1}{d #2}", 2], "pp": ["\\frac{\\partial #1}{\\partial #2}", 2], "norm": ["\\left\\lVert \\mathstrut #1 \\right\\rVert", 1], "abs": ["\\left\\lvert \\mathstrut #1 \\right\\rvert", 1], "twonorm": ["\\norm{#1}_2", 1], "onenorm": ["\\norm{#1}_1", 1], "infnorm": ["\\norm{#1}_\\infty", 1], "innerprod": ["\\langle #1,#2 \\rangle", 2], "pr": ["^{(#1)}", 1], "diag": ["\\operatorname{diag}"], "sign": ["\\operatorname{sign}"], "dist": ["\\operatorname{dist}"], "simil": ["\\operatorname{sim}"], "ee": ["\\times 10^"], "floor": ["\\lfloor#1\\rfloor", 1], "argmin": ["\\operatorname{argmin}"], "Cov": ["\\operatorname{Cov}"], "logit": ["\\operatorname{logit}"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.7. Exercises" href="exercises.html" />
    <link rel="prev" title="4.5. Probabilistic classification" href="prob_class.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<div class="col-12 col-md-3 bd-sidebar site-navigation " id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science 1</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Data Science 1 @ UD Math
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../representation/overview.html">
   1. Representation of data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/data-types.html">
     1.1. Types of data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/numpy.html">
     1.2. Introduction to numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/pandas.html">
     1.3. Introduction to pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/seaborn.html">
     1.4. Introduction to seaborn
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../statistics/overview.html">
   2. Descriptive statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/summary.html">
     2.1. Summary statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/split-apply-combine.html">
     2.2. Split–apply–combine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/outliers.html">
     2.3. Outliers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/correlation.html">
     2.4. Correlation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/exercises.html">
     2.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../classification/overview.html">
   3. Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/sklearn.html">
     3.1. Using scikit-learn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/performance.html">
     3.2. Classifier performance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/decision-trees.html">
     3.3. Decision trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/nearest-neighbors.html">
     3.4. Nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/svm.html">
     3.5. Support vector machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/overfitting.html">
     3.6. Overfitting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/model-selection.html">
     3.7. Model selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/exercises.html">
     3.8. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   4. Regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="linear.html">
     4.1. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multilinear.html">
     4.2. Multilinear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regularization.html">
     4.3. Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nonlinear.html">
     4.4. Nonlinear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prob_class.html">
     4.5. Probabilistic classification
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.6. Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="exercises.html">
     4.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../clustering/overview.html">
   5. Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../clustering/similarity.html">
     5.1. Similarity and distance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../clustering/performance.html">
     5.2. Clustering performance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../clustering/k-means.html">
     5.3. k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../clustering/hierarchical.html">
     5.4. Hierarchical clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../clustering/dbscan.html">
     5.5. DBSCAN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../clustering/exercises.html">
     5.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../network/overview.html">
   6. Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/networkx.html">
     6.1. Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/clustering.html">
     6.2. Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/distance.html">
     6.3. Distance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/degree.html">
     6.4. Degree distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/centrality.html">
     6.5. Centrality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/community.html">
     6.6. Communities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/exercises.html">
     6.7. Exercises
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<!-- This is an invisible pixel that we watch to see if we've scrolled. -->
<div class="sbt-scroll-pixel-helper"></div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            <div class="topbar-left">
                
                <label class="nav-toggle-button" for="__navigation">
                    <div class="visually-hidden">Toggle navigation</div>
                    <i class="fas fa-bars"></i>
                </label>
                
            </div>
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/regression/logistic.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/regression/logistic.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/UD-Math-Data-Science-1/notes/main?urlpath=tree/regression/logistic.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-function">
   Loss function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   Regularization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#case-study-personal-spam-filter">
   Case study: Personal spam filter
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiclass-case">
   Multiclass case
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#case-study-gas-sensor-drift">
   Case study: Gas sensor drift
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Logistic regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-function">
   Loss function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   Regularization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#case-study-personal-spam-filter">
   Case study: Personal spam filter
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiclass-case">
   Multiclass case
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#case-study-gas-sensor-drift">
   Case study: Gas sensor drift
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="logistic-regression">
<h1><span class="section-number">4.6. </span>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h1>
<p>The reinterpretation of classification methods as a form of regression on probability quickly leads to the question of looking for other ways to perform that regression. Specifically, can linear regression be adapted to that purpose? The answer is a qualified “yes”.</p>
<p>A linear regressor is the function <span class="math notranslate nohighlight">\(f(\bfx) = \bfx^T \bfw\)</span> for a constant vector <span class="math notranslate nohighlight">\(\bfw\)</span> (where we may augment <span class="math notranslate nohighlight">\(\bfx\)</span> with a constant in order to incorporate the intercept). It’s not a good candidate for representing a probability, which should vary between 0 and 1. A simple remedy is to transform its output using the <strong>logistic function</strong>, which is defined as</p>
<div class="math notranslate nohighlight">
\[
\sigma(x) = \frac{1}{1+e^{-x}}.
\]</div>
<div class="figure align-default">
<img alt="../_images/logistic.png" src="../_images/logistic.png" />
</div>
<p>The logistic function has the real line as its domain and takes the form of a smoothed step up from 0 to 1. Its inverse is the <strong>logit function</strong>,</p>
<div class="math notranslate nohighlight">
\[
\logit(p) = \ln\left( \frac{p}{1-p} \right).
\]</div>
<div class="figure align-default">
<img alt="../_images/logit.png" src="../_images/logit.png" />
</div>
<p>When interpreting <span class="math notranslate nohighlight">\(p\)</span> as probability, <span class="math notranslate nohighlight">\(\logit(p)\)</span> is the <strong>log-odds ratio</strong>. For instance, if <span class="math notranslate nohighlight">\(p=2/3\)</span>, then the odds ratio is <span class="math notranslate nohighlight">\((2/3)/(1/3)=2\)</span> (i.e., 2:1 odds), and <span class="math notranslate nohighlight">\(\logit(2/3)=\ln(2)\)</span>.</p>
<p>A natural use of linear regression, which has a range over all real numbers, is to match it to the logit of probability, rather than to probability itself:</p>
<div class="math notranslate nohighlight">
\[
\logit(p) \approx \bfx^T\bfw.
\]</div>
<p>This implies multilinear regression for the function <span class="math notranslate nohighlight">\(\logit(p)\)</span>, where <span class="math notranslate nohighlight">\(p\)</span> is the probability of the class <span class="math notranslate nohighlight">\(y=1\)</span>. Equivalently,</p>
<div class="math notranslate nohighlight">
\[
p \approx \sigma(\bfx^T\bfw).
\]</div>
<p>The resulting method is called <strong>logistic regression</strong>.</p>
<div class="section" id="loss-function">
<h2>Loss function<a class="headerlink" href="#loss-function" title="Permalink to this headline">¶</a></h2>
<p>At each training observation <span class="math notranslate nohighlight">\((\bfx_i,y_i)\)</span>, we know that either <span class="math notranslate nohighlight">\(y_i=0\)</span> or <span class="math notranslate nohighlight">\(y_i=1\)</span>. Extending the loss function for linear regression to the logistic case would suggest the minimization of least squares,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left[ \bfx_i^T\bfw - \logit(y_i) \right]^2. 
\]</div>
<p>However, the logits in this expression are all infinite, so a different loss function must be identified. One possibility is</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left[ \hat{p}_i - y_i \right]^2, \qquad \hat{p}_i = \sigma(\bfx_i^T\bfw) .
\]</div>
<p>It’s more common to minimize the <strong>cross-entropy</strong> loss function</p>
<div class="math notranslate nohighlight">
\[
L(\bfw) = -\sum_{i=1}^n \left[ y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right].
\]</div>
<p>The logarithms above can have any base, since that choice only changes <span class="math notranslate nohighlight">\(L\)</span> by a constant factor. Note that in cross-entropy, observation <span class="math notranslate nohighlight">\(i\)</span> contributes <span class="math notranslate nohighlight">\(-\log(1-\hat{p}_i)\)</span> if <span class="math notranslate nohighlight">\(y_i=0\)</span> and <span class="math notranslate nohighlight">\(-\log(\hat{p}_i)\)</span> if <span class="math notranslate nohighlight">\(y_i=1\)</span>. This loss function creates an unboundedly large penalty as <span class="math notranslate nohighlight">\(\hat{p}_i \to 1\)</span> if <span class="math notranslate nohighlight">\(y_i=0\)</span>, and vice versa, which often makes it preferable to the least-squares alternative above.</p>
<p>Logistic regression does have a major disadvantage compared to (multi)linear regression: the minimization of loss does <em>not</em> lead to a linear problem for the weight vector <span class="math notranslate nohighlight">\(\bfw\)</span>. The difference in practice is usually not concerning, though.</p>
</div>
<div class="section" id="regularization">
<h2>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">¶</a></h2>
<p>As with other forms of regression, the loss function may be regularized using the ridge or LASSO penalty. The standard formulation is</p>
<div class="math notranslate nohighlight">
\[
\widetilde{L}(\bfw) = C \, L(\bfw) + \norm{\bfw},
\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is a positive hyperparameter and the vector norm is either the 2-norm (ridge) or 1-norm (LASSO). Note that <span class="math notranslate nohighlight">\(C\)</span> functions like the inverse of the regularization parameter <span class="math notranslate nohighlight">\(\alpha\)</span> in our linear regressor. This is simply a different convention (like the one for the SVM), but it means that smaller values of <span class="math notranslate nohighlight">\(C\)</span> imply <em>greater</em> amounts of regularization.</p>
</div>
<div class="section" id="case-study-personal-spam-filter">
<h2>Case study: Personal spam filter<a class="headerlink" href="#case-study-personal-spam-filter" title="Permalink to this headline">¶</a></h2>
<p>We will try logistic regression for a simple spam filter. The data set is based on work and personal emails for one individual. The features are calculated word and character frequencies, as well as the appearance of capital letters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">spam</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;spambase.csv&quot;</span><span class="p">)</span>
<span class="n">spam</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word_freq_make</th>
      <th>word_freq_address</th>
      <th>word_freq_all</th>
      <th>word_freq_3d</th>
      <th>word_freq_our</th>
      <th>word_freq_over</th>
      <th>word_freq_remove</th>
      <th>word_freq_internet</th>
      <th>word_freq_order</th>
      <th>word_freq_mail</th>
      <th>...</th>
      <th>char_freq_%3B</th>
      <th>char_freq_%28</th>
      <th>char_freq_%5B</th>
      <th>char_freq_%21</th>
      <th>char_freq_%24</th>
      <th>char_freq_%23</th>
      <th>capital_run_length_average</th>
      <th>capital_run_length_longest</th>
      <th>capital_run_length_total</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00</td>
      <td>0.64</td>
      <td>0.64</td>
      <td>0.0</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0</td>
      <td>0.778</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>3.756</td>
      <td>61</td>
      <td>278</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.21</td>
      <td>0.28</td>
      <td>0.50</td>
      <td>0.0</td>
      <td>0.14</td>
      <td>0.28</td>
      <td>0.21</td>
      <td>0.07</td>
      <td>0.00</td>
      <td>0.94</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.132</td>
      <td>0.0</td>
      <td>0.372</td>
      <td>0.180</td>
      <td>0.048</td>
      <td>5.114</td>
      <td>101</td>
      <td>1028</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.06</td>
      <td>0.00</td>
      <td>0.71</td>
      <td>0.0</td>
      <td>1.23</td>
      <td>0.19</td>
      <td>0.19</td>
      <td>0.12</td>
      <td>0.64</td>
      <td>0.25</td>
      <td>...</td>
      <td>0.010</td>
      <td>0.143</td>
      <td>0.0</td>
      <td>0.276</td>
      <td>0.184</td>
      <td>0.010</td>
      <td>9.821</td>
      <td>485</td>
      <td>2259</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.63</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.137</td>
      <td>0.0</td>
      <td>0.137</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>3.537</td>
      <td>40</td>
      <td>191</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.63</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.135</td>
      <td>0.0</td>
      <td>0.135</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>3.537</td>
      <td>40</td>
      <td>191</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4596</th>
      <td>0.31</td>
      <td>0.00</td>
      <td>0.62</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.232</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1.142</td>
      <td>3</td>
      <td>88</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4597</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0</td>
      <td>0.353</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1.555</td>
      <td>4</td>
      <td>14</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4598</th>
      <td>0.30</td>
      <td>0.00</td>
      <td>0.30</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>...</td>
      <td>0.102</td>
      <td>0.718</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1.404</td>
      <td>6</td>
      <td>118</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4599</th>
      <td>0.96</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.057</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1.147</td>
      <td>5</td>
      <td>78</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4600</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.65</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0</td>
      <td>0.125</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1.250</td>
      <td>5</td>
      <td>40</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>4601 rows × 58 columns</p>
</div></div></div>
</div>
<p>We’ll create a feature matrix and label vector, and split into train/test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">spam</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">spam</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_tr</span><span class="p">,</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_tr</span><span class="p">,</span><span class="n">y_te</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When using norm-based regularization, it’s good practice to standardize the variables, so we will prepare to set up a pipeline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<p>First we use a large value of <span class="math notranslate nohighlight">\(C\)</span> to emphasize the regressive loss rather than the regularization penalty. (The default regularization norm is the 2-norm.) It’s not required to select a solver, but we choose one here that is reliable for small data sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">logr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">)</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">logr</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy:&quot;</span><span class="p">,</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>accuracy: 0.9337676438653637
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the most extreme regression coefficients, associating them with the feature names and then sorting the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">logr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;least spammy:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coef</span><span class="p">[:</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">most spammy:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>least spammy:
word_freq_george    -24.055873
word_freq_cs         -8.573934
word_freq_hp         -3.512677
word_freq_meeting    -1.940974
dtype: float64

most spammy:
char_freq_%23                 1.155145
char_freq_%24                 1.262550
capital_run_length_longest    1.975006
word_freq_3d                  2.112929
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>The word “george” is a strong counter-indicator for spam (remember that this data set comes from an individual). Its presence makes the inner product <span class="math notranslate nohighlight">\(\bfx^T\bfw\)</span> more negative, which drives the logistic function closer to 0. Conversely, the presence of consecutive capital letters increases the inner product and pushes the probability closer to 1.</p>
<p>The predictions by the regressor are all either 0 or 1. But we can also see the forecasted probabilities before thresholding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;predicted classes:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_tr</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">5</span><span class="p">,:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">probabilities:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_tr</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">5</span><span class="p">,:]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>predicted classes:
[0 0 0 0 0]

probabilities:
[[0.53769264 0.46230736]
 [0.99694715 0.00305285]
 [0.63975661 0.36024339]
 [0.996342   0.003658  ]
 [0.93740435 0.06259565]]
</pre></div>
</div>
</div>
</div>
<p>The probabilities might be useful for making decisions based on the results. For example, the first instance above was much less certain about the classification than the second, and a lower threshold for determining spam might have changed the class to 1. The probability matrix can be used to create an ROC curve showing the tradeoffs over all thresholds.</p>
<p>For a validation-based selection of the best regularization parameter value, we can use <code class="docutils literal notranslate"><span class="pre">LogisticRegressionCV</span></code>, which is basically a convenience method for a grid search. You can specify which values of <span class="math notranslate nohighlight">\(C\)</span> to search over, or just say how many, as we do here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="n">logr</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">Cs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">logr</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best C value: </span><span class="si">{</span><span class="n">logr</span><span class="o">.</span><span class="n">C_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3g</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;accuracy score: </span><span class="si">{</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>best C value: 21.5
accuracy score: 0.9349
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="multiclass-case">
<h2>Multiclass case<a class="headerlink" href="#multiclass-case" title="Permalink to this headline">¶</a></h2>
<p>When there are more than two unique labels possible, logistic regression can be extended through the <strong>one-vs-rest</strong> (OVR) paradigm. Given <span class="math notranslate nohighlight">\(K\)</span> classes, there are <span class="math notranslate nohighlight">\(K\)</span> binary regressors fit for the outcomes “class 1/not class 1,” “class 2/not class 2,” and so on, giving <span class="math notranslate nohighlight">\(K\)</span> different coefficient vectors, <span class="math notranslate nohighlight">\(\bfw_k\)</span>. Now for a sample point <span class="math notranslate nohighlight">\(\bfx_i\)</span> we predict probabilities for it being in each class:</p>
<div class="math notranslate nohighlight">
\[
\hat{q}_{i,k} = \sigma(\bfx_i^T \bfw_k), \qquad k=1,\ldots,K. 
\]</div>
<p>Since the <span class="math notranslate nohighlight">\(K\)</span> OVR regressors are done independently, there is no reason to think these probabilities will sum to 1 over all the classes. But it’s easy to normalize them:</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_{i,k} = \frac{\hat{q}_{i,k}}{\sum_{k=1}^K \hat{q}_{i,k}}.
\]</div>
<p>That is, we get a matrix of probabilities. Each of the <span class="math notranslate nohighlight">\(n\)</span> rows gives the class probabilities at a single sample point, and each of the <span class="math notranslate nohighlight">\(K\)</span> columns gives the probability of one class at all the samples.</p>
</div>
<div class="section" id="case-study-gas-sensor-drift">
<h2>Case study: Gas sensor drift<a class="headerlink" href="#case-study-gas-sensor-drift" title="Permalink to this headline">¶</a></h2>
<p>As a multiclass example, we use a data set about gas sensors recording values over long periods of time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gas</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;gas_drift.csv&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">gas</span><span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">gas</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
<span class="n">X_tr</span><span class="p">,</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_tr</span><span class="p">,</span><span class="n">y_te</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">logr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">)</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">logr</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy score:&quot;</span><span class="p">,</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>accuracy score: 0.98705966930266
</pre></div>
</div>
</div>
</div>
<p>We can now look at predictions of probability for each class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">p_hat</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Class &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">)])</span>
<span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class 1</th>
      <th>Class 2</th>
      <th>Class 3</th>
      <th>Class 4</th>
      <th>Class 5</th>
      <th>Class 6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000153</td>
      <td>5.335916e-06</td>
      <td>0.020479</td>
      <td>4.207813e-03</td>
      <td>0.003191</td>
      <td>9.719636e-01</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000004</td>
      <td>9.999199e-01</td>
      <td>0.000074</td>
      <td>6.812814e-07</td>
      <td>0.000002</td>
      <td>5.883007e-17</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.008648</td>
      <td>3.856552e-03</td>
      <td>0.000017</td>
      <td>1.282034e-03</td>
      <td>0.001294</td>
      <td>9.849029e-01</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.237644</td>
      <td>5.483765e-08</td>
      <td>0.000020</td>
      <td>7.221450e-01</td>
      <td>0.000054</td>
      <td>4.013725e-02</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.016620</td>
      <td>3.644173e-02</td>
      <td>0.010126</td>
      <td>2.032038e-01</td>
      <td>0.032094</td>
      <td>7.015147e-01</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2777</th>
      <td>0.000002</td>
      <td>9.045888e-01</td>
      <td>0.060927</td>
      <td>2.871603e-03</td>
      <td>0.031522</td>
      <td>8.949040e-05</td>
    </tr>
    <tr>
      <th>2778</th>
      <td>0.000089</td>
      <td>1.782929e-04</td>
      <td>0.001882</td>
      <td>7.461637e-04</td>
      <td>0.017699</td>
      <td>9.794063e-01</td>
    </tr>
    <tr>
      <th>2779</th>
      <td>0.000254</td>
      <td>2.038618e-05</td>
      <td>0.004423</td>
      <td>2.361770e-04</td>
      <td>0.024404</td>
      <td>9.706624e-01</td>
    </tr>
    <tr>
      <th>2780</th>
      <td>0.009284</td>
      <td>4.194312e-08</td>
      <td>0.989908</td>
      <td>6.143468e-04</td>
      <td>0.000025</td>
      <td>1.676350e-04</td>
    </tr>
    <tr>
      <th>2781</th>
      <td>0.004260</td>
      <td>3.264299e-02</td>
      <td>0.007537</td>
      <td>9.340926e-01</td>
      <td>0.013999</td>
      <td>7.467973e-03</td>
    </tr>
  </tbody>
</table>
<p>2782 rows × 6 columns</p>
</div></div></div>
</div>
<p>Here is a look at how the maximum prediction probability for each row in the test set is distributed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">sns</span><span class="o">.</span><span class="n">displot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">p_hat</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/logistic_19_0.png" src="../_images/logistic_19_0.png" />
</div>
</div>
<p>You can see from the plot that a solid majority of classifications are made with at least 90% probability. So if we set a high threshold for classification, we should get few false positives while still getting good recall. An AUC-ROC score can be computed by averaging the values over the curves for each class. In this case, AUC-ROC score is very high:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">p_hat</span><span class="p">,</span><span class="n">multi_class</span><span class="o">=</span><span class="s2">&quot;ovr&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9985961583670022
</pre></div>
</div>
</div>
</div>
<div style="max-width:608px"><div style="position:relative;padding-bottom:66.118421052632%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_mr2gh70i&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_en335xwu" width="608" height="402" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player" style="position:absolute;top:0;left:0;width:100%;height:100%"></iframe></div></div></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "UD-Math-Data-Science-1/notes",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="prob_class.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">4.5. </span>Probabilistic classification</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="exercises.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4.7. </span>Exercises</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tobin A. Driscoll<br/>
    
        &copy; Copyright 2022.<br/>
      <div class="extra_footer">
        <img alt='UD logo' src='_static/UDlogo-small.png'>
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>