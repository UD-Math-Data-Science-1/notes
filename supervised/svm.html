
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3.5. Support vector machine &#8212; Data Science 1</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"float": ["\\mathbb{F}"], "real": ["\\mathbb{R}"], "complex": ["\\mathbb{C}"], "nat": ["\\mathbb{N}"], "integer": ["\\mathbb{Z}"], "bfa": "\\mathbf{a}", "bfe": "\\mathbf{e}", "bfx": "\\mathbf{x}", "bfX": "\\mathbf{X}", "bfw": "\\mathbf{w}", "bfy": "\\mathbf{y}", "bfz": "\\mathbf{z}", "bfzero": "\\boldsymbol{0}", "rmn": ["\\mathbb{R}^{#1 \\times #2}", 2], "dd": ["\\frac{d #1}{d #2}", 2], "pp": ["\\frac{\\partial #1}{\\partial #2}", 2], "norm": ["\\| #1 \\|", 1], "twonorm": ["\\| #1 \\|_2", 1], "onenorm": ["\\| #1 \\|_1", 1], "infnorm": ["\\| #1 \\|_\\infty", 1], "innerprod": ["\\langle #1,#2 \\rangle", 2], "pr": ["^{(#1)}", 1], "diag": ["\\operatorname{diag}"], "sign": ["\\operatorname{sign}"], "ee": ["\\times 10^"], "floor": ["\\lfloor#1\\rfloor", 1], "argmin": ["\\operatorname{argmin}"], "Cov": ["\\operatorname{Cov}"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.6. Model selection" href="model-selection.html" />
    <link rel="prev" title="3.4. Nearest neighbors" href="nearest-neighbors.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science 1</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Data Science 1 @ UD Math
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../representation/overview.html">
   1. Representation of data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/data-types.html">
     1.1. Types of data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/numpy.html">
     1.2. Introduction to numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/pandas.html">
     1.3. Introduction to pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/seaborn.html">
     1.4. Introduction to seaborn
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../statistics/overview.html">
   2. Descriptive statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/split-apply-combine.html">
     2.1. Split–apply–combine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/summary.html">
     2.2. Summary statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/outliers.html">
     2.3. Outliers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/correlation.html">
     2.4. Correlation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   3. Classification
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="scikit.html">
     3.1. Using scikit-learn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="performance.html">
     3.2. Classifier performance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="decision-trees.html">
     3.3. Decision trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nearest-neighbors.html">
     3.4. Nearest neighbors
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.5. Support vector machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model-selection.html">
     3.6. Model selection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../regression/overview.html">
   4. Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/linear.html">
     4.1. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/multilinear.html">
     4.2. Multilinear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/regularization.html">
     4.3. Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/logistic.html">
     4.4. Logistic regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unsupervised/overview.html">
   5. Unsupervised learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised/preliminaries.html">
     5.1. Preliminaries
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised/k-means.html">
     5.2. k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised/hierarchical.html">
     5.3. Hierarchical
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/supervised/svm.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/supervised/svm.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/UD-Math-Data-Science-1/notes/main?urlpath=tree/supervised/svm.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#higher-dimensions">
   Higher dimensions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advanced-aspects">
   Advanced aspects
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#usage-in-sklearn">
   Usage in sklearn
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <!-- Table of contents that is only displayed when printing the page -->
    <div id="jb-print-docs-body" class="onlyprint">
        <h1>Support vector machine</h1>
        <!-- Table of contents -->
        <div id="print-main-content" class="row">
            <div class="col-12 col-md-12 pl-md-5 pr-md-5">
            <div id="jb-print-toc">
                
                <div>
                    <h2> Contents </h2>
                </div>
                <nav aria-label="Page">
                    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#higher-dimensions">
   Higher dimensions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advanced-aspects">
   Advanced aspects
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#usage-in-sklearn">
   Usage in sklearn
  </a>
 </li>
</ul>

                </nav>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="support-vector-machine">
<h1><span class="section-number">3.5. </span>Support vector machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h1>
<p>Suppose for a moment that we have data samples with just two dimensions (features), and that they are arranged like this:</p>
<div class="figure align-default">
<img alt="../_images/svm_many.png" src="../_images/svm_many.png" />
</div>
<p>These groups are easy to separate! In fact, as the figure shows, we are spoiled for choice, and the possibilities span a wide range, even if we use only straight lines.</p>
<p>However, there is a way to define the <em>best</em> line separating the two sets, as illustrated in this figure:</p>
<div class="figure align-default">
<img alt="../_images/svm_margins.png" src="../_images/svm_margins.png" />
</div>
<p>The key is to define the <strong>margin</strong> between the sets as the maximum perpendicular distance between the points and the line. It turns out that, if the sets can be separated by a line at all, then there is a line that maximizes the margin. It’s also the case that only a few of the sample points actually matter, as shown by the boxes in the figure. They are the ones that achieve the margins and are called <strong>support vectors</strong>. A learner based on this idea of separating data by maximum margin is called a <strong>support vector machine</strong> (SVM).</p>
<p>Let’s express the line and margin mathematically in two dimensions. We are used to writing a line as <span class="math notranslate nohighlight">\(y=mx+b\)</span>. First, though, we are going to use subscripts rather than letters for the dimensions, so make that <span class="math notranslate nohighlight">\(x_2=mx_1+b\)</span>. Next, we recall that this equation doesn’t work for vertical lines (infinite slope), so we need a coefficient in front of <span class="math notranslate nohighlight">\(x_2\)</span> as well. Rearranging, we get</p>
<div class="math notranslate nohighlight">
\[
w_1 x_1 + w_2 x_2 + b = 0,
\]</div>
<p>where <span class="math notranslate nohighlight">\(w_1,w_2,b\)</span> are constants.</p>
<p>Our next observation is that if the point <span class="math notranslate nohighlight">\((a_1,a_2)\)</span> is on the line, then any point of the form</p>
<div class="math notranslate nohighlight">
\[
x_1 = a_1 - tw_2, \quad x_2 = a_2 + tw_1
\]</div>
<p>is also on the line. (Just substitute it in to see that it satisfies the equation of the line.)</p>
<p>Now let us find the distance from any point <span class="math notranslate nohighlight">\((s_1,s_2)\)</span> to the line. The distance squared from this point to an arbitrary line point is</p>
<div class="math notranslate nohighlight">
\[
d^2 = (s_1-a_1+tw_2)^2 + (s2-a_2-tw_1)^2.
\]</div>
<p>Using calculus to minimize <span class="math notranslate nohighlight">\(d^2\)</span> as a function of <span class="math notranslate nohighlight">\(t\)</span> eventually gives</p>
<div class="math notranslate nohighlight">
\[
t_\text{min} = \frac{(s_2-a_2)w_1-(s_1-a_1)w_2}{w_1^2+w^2^2}.
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(w_1^2+w_2^2=\|[w_1,w_2]\|_2^2\)</span>. Substituting <span class="math notranslate nohighlight">\(t_\text{min}\)</span> into <span class="math notranslate nohighlight">\(d^2\)</span> and taking a square root gives</p>
<div class="math notranslate nohighlight">
\[
d_\text{min} = \frac{|(s_1-a_1)w_1 + (s_2-a_2)w_2|}{\|\bfw\|^2}
 = \frac{|(s_1w_1 + s_2w_2 + b|}{\|\bfw\|_2}.
\]</div>
<p>It is clear that <span class="math notranslate nohighlight">\(w_1 s_1 + w_2 s_2 + b &gt; 0\)</span> represents the half-plane on one side of the line, and $<span class="math notranslate nohighlight">\(w_1 s_1 + w_2 s_2 + b &lt; 0\)</span> represents the other. Suppose we use <span class="math notranslate nohighlight">\(y_i=+1\)</span> for all the labels on one side, and <span class="math notranslate nohighlight">\(y_i=-1\)</span> for all the labels on the other side. Finally, the condition that the distance from the line to point <span class="math notranslate nohighlight">\((x_{i,1},x_{i,2})\)</span> be no smaller than the margin <span class="math notranslate nohighlight">\(M\)</span>, and that the point be on the correct side of the line, is</p>
<div class="math notranslate nohighlight">
\[
y_i\left( \frac{ s_1w_1 + s_2w_2 + b }{\|\bfw\|_2} \right) \ge M,
\]</div>
<p>which must hold true for all <span class="math notranslate nohighlight">\(i\)</span> as we maximize the margin <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>This is a <em>constrained optimization</em> problem. The details of how it’s solved are interesting, but beyond us in this space.</p>
<div class="section" id="higher-dimensions">
<h2>Higher dimensions<a class="headerlink" href="#higher-dimensions" title="Permalink to this headline">¶</a></h2>
<p>What happens in <span class="math notranslate nohighlight">\(d&gt;2\)</span> dimensions? Instead of a line, we have a <strong>hyperplane</strong> of dimension <span class="math notranslate nohighlight">\(d-1\)</span>. Its equation can be expressed as</p>
<div class="math notranslate nohighlight">
\[
w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b = 0,
\]</div>
<p>for some constants <span class="math notranslate nohighlight">\(w_1,\ldots,w_d,b\)</span>. In fact, the vector <span class="math notranslate nohighlight">\(\bfw=[w_1,\ldots,w_d]\)</span> is said to be perpendicular or <strong>normal</strong> to the hyperplane.</p>
<p>We have the important new notation</p>
<div class="math notranslate nohighlight">
\[
\bfw^T\bfx = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d 
\]</div>
<p>as the <strong>inner product</strong> between vectors <span class="math notranslate nohighlight">\(\bfx\)</span> and <span class="math notranslate nohighlight">\(\bfw\)</span>. It follows easily that</p>
<div class="math notranslate nohighlight">
\[
\bfw^T\bfw = \|\bfw\|_2^2,
\]</div>
<p>which is the important fact that makes the 2-norm special. One form of the constrained optimization problem (known as the <em>primal formulation</em>) is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{minimize } &amp; \|\bfw\|_2 \\ 
\text{subject to } &amp; y_i(\bfw^T \bfx_i + b) \ge 1,\, i = 1,\ldots,n.
\end{split}\]</div>
<p>Usually, though, the optimization is actually performed on an equivalent <em>dual formulation</em>, which finds the <span class="math notranslate nohighlight">\(d+1\)</span> support vectors and the margin rather than <span class="math notranslate nohighlight">\(\bfw\)</span> and <span class="math notranslate nohighlight">\(b\)</span> directly.</p>
</div>
<div class="section" id="advanced-aspects">
<h2>Advanced aspects<a class="headerlink" href="#advanced-aspects" title="Permalink to this headline">¶</a></h2>
<p>There are other refinements too advanced for us to go into in detail here. Two stand out for making the algorithm practical for more than a trivial number of problems.</p>
<p>One is the idea of allowing <em>slack</em>, which means that points are allowed to be on the wrong side of the dividing hyperplane, but they are punished by an amount proportional to their distance from it. The balance between maximizing margin and punishing miscreants is controlled by a hyperparameter that is usually called <span class="math notranslate nohighlight">\(C\)</span>, and the algorithm may be called <strong>C-SVM</strong>.</p>
<p>The other important refinement is to upgrade the separating hyperplane to allow other kinds of surfaces. For reasons we won’t go into, this is called the <strong>kernel trick</strong>, and specifying the kernel is another option. The most common choices are <em>linear</em>, which is the original hyperplane, and <em>RBF</em>, which has its own hyperparameter <span class="math notranslate nohighlight">\(\gamma\)</span>.</p>
</div>
<div class="section" id="usage-in-sklearn">
<h2>Usage in sklearn<a class="headerlink" href="#usage-in-sklearn" title="Permalink to this headline">¶</a></h2>
<p>The SVM classifier in sklearn is called <code class="docutils literal notranslate"><span class="pre">SVC</span></code>. By default, it uses <span class="math notranslate nohighlight">\(C=1\)</span> and the RBF kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;data.csv&quot;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;labels.csv&quot;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">X_tr</span><span class="p">,</span> <span class="n">X_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train accuracy:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train accuracy: 0.8173309178743962
test accuracy: 0.8176328502415459
</pre></div>
</div>
</div>
</div>
<p>The training accuracy essentially tells us how much slack was allowed; i.e., how frequently sample points end up on the wrong side of the decision boundary. If we increase <span class="math notranslate nohighlight">\(C\)</span>, we penalize the slack more and increase the training accuracy. (That might increase the test error as well, but we explore that relationship more in the next section.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train accuracy with less slack:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy with less slack:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">yhat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train accuracy with less slack: 0.8535628019323671
test accuracy with less slack: 0.8502415458937198
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[677   0]
 [124  27]]
</pre></div>
</div>
</div>
</div>
<p>SVM usually benefits from standardizing the features, so it’s a good idea to build that in.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>   

<span class="n">svc</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">())</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train accuracy with standardization:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy with standardization:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train accuracy with standardization: 0.8457125603864735
test accuracy with standardization: 0.8417874396135265
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "UD-Math-Data-Science-1/notes",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./supervised"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </div>
        </div>
    </div>
    <div id="main-content" class="row noprint">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="support-vector-machine">
<h1><span class="section-number">3.5. </span>Support vector machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h1>
<p>Suppose for a moment that we have data samples with just two dimensions (features), and that they are arranged like this:</p>
<div class="figure align-default">
<img alt="../_images/svm_many.png" src="../_images/svm_many.png" />
</div>
<p>These groups are easy to separate! In fact, as the figure shows, we are spoiled for choice, and the possibilities span a wide range, even if we use only straight lines.</p>
<p>However, there is a way to define the <em>best</em> line separating the two sets, as illustrated in this figure:</p>
<div class="figure align-default">
<img alt="../_images/svm_margins.png" src="../_images/svm_margins.png" />
</div>
<p>The key is to define the <strong>margin</strong> between the sets as the maximum perpendicular distance between the points and the line. It turns out that, if the sets can be separated by a line at all, then there is a line that maximizes the margin. It’s also the case that only a few of the sample points actually matter, as shown by the boxes in the figure. They are the ones that achieve the margins and are called <strong>support vectors</strong>. A learner based on this idea of separating data by maximum margin is called a <strong>support vector machine</strong> (SVM).</p>
<p>Let’s express the line and margin mathematically in two dimensions. We are used to writing a line as <span class="math notranslate nohighlight">\(y=mx+b\)</span>. First, though, we are going to use subscripts rather than letters for the dimensions, so make that <span class="math notranslate nohighlight">\(x_2=mx_1+b\)</span>. Next, we recall that this equation doesn’t work for vertical lines (infinite slope), so we need a coefficient in front of <span class="math notranslate nohighlight">\(x_2\)</span> as well. Rearranging, we get</p>
<div class="math notranslate nohighlight">
\[
w_1 x_1 + w_2 x_2 + b = 0,
\]</div>
<p>where <span class="math notranslate nohighlight">\(w_1,w_2,b\)</span> are constants.</p>
<p>Our next observation is that if the point <span class="math notranslate nohighlight">\((a_1,a_2)\)</span> is on the line, then any point of the form</p>
<div class="math notranslate nohighlight">
\[
x_1 = a_1 - tw_2, \quad x_2 = a_2 + tw_1
\]</div>
<p>is also on the line. (Just substitute it in to see that it satisfies the equation of the line.)</p>
<p>Now let us find the distance from any point <span class="math notranslate nohighlight">\((s_1,s_2)\)</span> to the line. The distance squared from this point to an arbitrary line point is</p>
<div class="math notranslate nohighlight">
\[
d^2 = (s_1-a_1+tw_2)^2 + (s2-a_2-tw_1)^2.
\]</div>
<p>Using calculus to minimize <span class="math notranslate nohighlight">\(d^2\)</span> as a function of <span class="math notranslate nohighlight">\(t\)</span> eventually gives</p>
<div class="math notranslate nohighlight">
\[
t_\text{min} = \frac{(s_2-a_2)w_1-(s_1-a_1)w_2}{w_1^2+w^2^2}.
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(w_1^2+w_2^2=\|[w_1,w_2]\|_2^2\)</span>. Substituting <span class="math notranslate nohighlight">\(t_\text{min}\)</span> into <span class="math notranslate nohighlight">\(d^2\)</span> and taking a square root gives</p>
<div class="math notranslate nohighlight">
\[
d_\text{min} = \frac{|(s_1-a_1)w_1 + (s_2-a_2)w_2|}{\|\bfw\|^2}
 = \frac{|(s_1w_1 + s_2w_2 + b|}{\|\bfw\|_2}.
\]</div>
<p>It is clear that <span class="math notranslate nohighlight">\(w_1 s_1 + w_2 s_2 + b &gt; 0\)</span> represents the half-plane on one side of the line, and $<span class="math notranslate nohighlight">\(w_1 s_1 + w_2 s_2 + b &lt; 0\)</span> represents the other. Suppose we use <span class="math notranslate nohighlight">\(y_i=+1\)</span> for all the labels on one side, and <span class="math notranslate nohighlight">\(y_i=-1\)</span> for all the labels on the other side. Finally, the condition that the distance from the line to point <span class="math notranslate nohighlight">\((x_{i,1},x_{i,2})\)</span> be no smaller than the margin <span class="math notranslate nohighlight">\(M\)</span>, and that the point be on the correct side of the line, is</p>
<div class="math notranslate nohighlight">
\[
y_i\left( \frac{ s_1w_1 + s_2w_2 + b }{\|\bfw\|_2} \right) \ge M,
\]</div>
<p>which must hold true for all <span class="math notranslate nohighlight">\(i\)</span> as we maximize the margin <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>This is a <em>constrained optimization</em> problem. The details of how it’s solved are interesting, but beyond us in this space.</p>
<div class="section" id="higher-dimensions">
<h2>Higher dimensions<a class="headerlink" href="#higher-dimensions" title="Permalink to this headline">¶</a></h2>
<p>What happens in <span class="math notranslate nohighlight">\(d&gt;2\)</span> dimensions? Instead of a line, we have a <strong>hyperplane</strong> of dimension <span class="math notranslate nohighlight">\(d-1\)</span>. Its equation can be expressed as</p>
<div class="math notranslate nohighlight">
\[
w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b = 0,
\]</div>
<p>for some constants <span class="math notranslate nohighlight">\(w_1,\ldots,w_d,b\)</span>. In fact, the vector <span class="math notranslate nohighlight">\(\bfw=[w_1,\ldots,w_d]\)</span> is said to be perpendicular or <strong>normal</strong> to the hyperplane.</p>
<p>We have the important new notation</p>
<div class="math notranslate nohighlight">
\[
\bfw^T\bfx = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d 
\]</div>
<p>as the <strong>inner product</strong> between vectors <span class="math notranslate nohighlight">\(\bfx\)</span> and <span class="math notranslate nohighlight">\(\bfw\)</span>. It follows easily that</p>
<div class="math notranslate nohighlight">
\[
\bfw^T\bfw = \|\bfw\|_2^2,
\]</div>
<p>which is the important fact that makes the 2-norm special. One form of the constrained optimization problem (known as the <em>primal formulation</em>) is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{minimize } &amp; \|\bfw\|_2 \\ 
\text{subject to } &amp; y_i(\bfw^T \bfx_i + b) \ge 1,\, i = 1,\ldots,n.
\end{split}\]</div>
<p>Usually, though, the optimization is actually performed on an equivalent <em>dual formulation</em>, which finds the <span class="math notranslate nohighlight">\(d+1\)</span> support vectors and the margin rather than <span class="math notranslate nohighlight">\(\bfw\)</span> and <span class="math notranslate nohighlight">\(b\)</span> directly.</p>
</div>
<div class="section" id="advanced-aspects">
<h2>Advanced aspects<a class="headerlink" href="#advanced-aspects" title="Permalink to this headline">¶</a></h2>
<p>There are other refinements too advanced for us to go into in detail here. Two stand out for making the algorithm practical for more than a trivial number of problems.</p>
<p>One is the idea of allowing <em>slack</em>, which means that points are allowed to be on the wrong side of the dividing hyperplane, but they are punished by an amount proportional to their distance from it. The balance between maximizing margin and punishing miscreants is controlled by a hyperparameter that is usually called <span class="math notranslate nohighlight">\(C\)</span>, and the algorithm may be called <strong>C-SVM</strong>.</p>
<p>The other important refinement is to upgrade the separating hyperplane to allow other kinds of surfaces. For reasons we won’t go into, this is called the <strong>kernel trick</strong>, and specifying the kernel is another option. The most common choices are <em>linear</em>, which is the original hyperplane, and <em>RBF</em>, which has its own hyperparameter <span class="math notranslate nohighlight">\(\gamma\)</span>.</p>
</div>
<div class="section" id="usage-in-sklearn">
<h2>Usage in sklearn<a class="headerlink" href="#usage-in-sklearn" title="Permalink to this headline">¶</a></h2>
<p>The SVM classifier in sklearn is called <code class="docutils literal notranslate"><span class="pre">SVC</span></code>. By default, it uses <span class="math notranslate nohighlight">\(C=1\)</span> and the RBF kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;data.csv&quot;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;labels.csv&quot;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">X_tr</span><span class="p">,</span> <span class="n">X_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train accuracy:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train accuracy: 0.8173309178743962
test accuracy: 0.8176328502415459
</pre></div>
</div>
</div>
</div>
<p>The training accuracy essentially tells us how much slack was allowed; i.e., how frequently sample points end up on the wrong side of the decision boundary. If we increase <span class="math notranslate nohighlight">\(C\)</span>, we penalize the slack more and increase the training accuracy. (That might increase the test error as well, but we explore that relationship more in the next section.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train accuracy with less slack:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy with less slack:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">yhat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train accuracy with less slack: 0.8535628019323671
test accuracy with less slack: 0.8502415458937198
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[677   0]
 [124  27]]
</pre></div>
</div>
</div>
</div>
<p>SVM usually benefits from standardizing the features, so it’s a good idea to build that in.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>   

<span class="n">svc</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">())</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train accuracy with standardization:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span><span class="n">y_tr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy with standardization:&quot;</span><span class="p">,</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span><span class="n">y_te</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train accuracy with standardization: 0.8457125603864735
test accuracy with standardization: 0.8417874396135265
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "UD-Math-Data-Science-1/notes",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./supervised"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="nearest-neighbors.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3.4. </span>Nearest neighbors</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="model-selection.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3.6. </span>Model selection</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tobin A. Driscoll<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>