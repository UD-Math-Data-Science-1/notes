
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5.1. Similarity and distance &#8212; Data Science 1</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=84ace793992934648b4de8eed757e5a2" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.9d8b4a8b9bb19db25eeaddc40d639ba2.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"float": ["\\mathbb{F}"], "real": ["\\mathbb{R}"], "complex": ["\\mathbb{C}"], "nat": ["\\mathbb{N}"], "integer": ["\\mathbb{Z}"], "bfa": "\\mathbf{a}", "bfe": "\\mathbf{e}", "bfx": "\\mathbf{x}", "bfX": "\\mathbf{X}", "bfu": "\\mathbf{u}", "bfv": "\\mathbf{v}", "bfw": "\\mathbf{w}", "bfy": "\\mathbf{y}", "bfz": "\\mathbf{z}", "bfzero": "\\boldsymbol{0}", "bfmu": "\\boldsymbol{\\mu}", "TP": "\\text{TP}", "TN": "\\text{TN}", "FP": "\\text{FP}", "FN": "\\text{FN}", "rmn": ["\\mathbb{R}^{#1 \\times #2}", 2], "dd": ["\\frac{d #1}{d #2}", 2], "pp": ["\\frac{\\partial #1}{\\partial #2}", 2], "norm": ["\\left\\lVert \\mathstrut #1 \\right\\rVert", 1], "abs": ["\\left\\lvert \\mathstrut #1 \\right\\rvert", 1], "twonorm": ["\\norm{#1}_2", 1], "onenorm": ["\\norm{#1}_1", 1], "infnorm": ["\\norm{#1}_\\infty", 1], "innerprod": ["\\langle #1,#2 \\rangle", 2], "pr": ["^{(#1)}", 1], "diag": ["\\operatorname{diag}"], "sign": ["\\operatorname{sign}"], "dist": ["\\operatorname{dist}"], "simil": ["\\operatorname{sim}"], "ee": ["\\times 10^"], "floor": ["\\lfloor#1\\rfloor", 1], "argmin": ["\\operatorname{argmin}"], "Cov": ["\\operatorname{Cov}"], "logit": ["\\operatorname{logit}"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5.2. Clustering performance" href="performance.html" />
    <link rel="prev" title="5. Clustering" href="overview.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<div class="col-12 col-md-3 bd-sidebar site-navigation " id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science 1</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Data Science 1 @ UD Math
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../representation/overview.html">
   1. Representation of data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/data-types.html">
     1.1. Types of data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/numpy.html">
     1.2. Introduction to numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/pandas.html">
     1.3. Introduction to pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../representation/seaborn.html">
     1.4. Introduction to seaborn
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../statistics/overview.html">
   2. Descriptive statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/summary.html">
     2.1. Summary statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/split-apply-combine.html">
     2.2. Split–apply–combine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/outliers.html">
     2.3. Outliers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/correlation.html">
     2.4. Correlation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/exercises.html">
     2.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../classification/overview.html">
   3. Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/sklearn.html">
     3.1. Using scikit-learn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/performance.html">
     3.2. Classifier performance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/decision-trees.html">
     3.3. Decision trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/nearest-neighbors.html">
     3.4. Nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/svm.html">
     3.5. Support vector machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/overfitting.html">
     3.6. Overfitting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/model-selection.html">
     3.7. Model selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/exercises.html">
     3.8. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../regression/overview.html">
   4. Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/linear.html">
     4.1. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/multilinear.html">
     4.2. Multilinear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/regularization.html">
     4.3. Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/nonlinear.html">
     4.4. Nonlinear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/prob_class.html">
     4.5. Probabilistic classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/logistic.html">
     4.6. Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression/exercises.html">
     4.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   5. Clustering
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5.1. Similarity and distance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="performance.html">
     5.2. Clustering performance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="k-means.html">
     5.3. k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hierarchical.html">
     5.4. Hierarchical
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dbscan.html">
     5.5. DBSCAN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="exercises.html">
     5.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../network/overview.html">
   6. Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/networkx.html">
     6.1. Basics of NetworkX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../network/degree.html">
     6.2. Degree and distance
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<!-- This is an invisible pixel that we watch to see if we've scrolled. -->
<div class="sbt-scroll-pixel-helper"></div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            <div class="topbar-left">
                
                <label class="nav-toggle-button" for="__navigation">
                    <div class="visually-hidden">Toggle navigation</div>
                    <i class="fas fa-bars"></i>
                </label>
                
            </div>
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/clustering/similarity.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/clustering/similarity.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/UD-Math-Data-Science-1/notes/main?urlpath=tree/clustering/similarity.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance-metrics">
   Distance metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-distributions">
   Probability distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance-matrix">
   Distance matrix
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#toy-example">
     Toy example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance-in-high-dimensions">
   Distance in high dimensions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Similarity and distance</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance-metrics">
   Distance metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-distributions">
   Probability distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance-matrix">
   Distance matrix
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#toy-example">
     Toy example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance-in-high-dimensions">
   Distance in high dimensions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="similarity-and-distance">
<h1><span class="section-number">5.1. </span>Similarity and distance<a class="headerlink" href="#similarity-and-distance" title="Permalink to this headline">¶</a></h1>
<p>Given an <span class="math notranslate nohighlight">\(n\times d\)</span> feature matrix, we want to define disjoint subsets of the <span class="math notranslate nohighlight">\(\bfx_i\)</span> such that the samples within a subset, or <strong>cluster</strong>, are more similar to one another than they are to members of other clusters.</p>
<p>The first decision we have to make is how to measure <em>similarity</em>. When a distance metric is available, we consider similarity to be inversely related to distance. For example, if we have defined a distance function between pairs of vectors as <span class="math notranslate nohighlight">\(\dist(\bfx,\bfy)\)</span>, then we could define similarity as</p>
<div class="math notranslate nohighlight">
\[
\simil(\bfx,\bfy) = \exp \left[ - \frac{\dist(\bfx,\bfy)^2}{2\sigma^2}  \right].
\]</div>
<p>Thus a distance of zero implies a similarity of 1, while the similarity tends to zero as distance increases. The scaling parameter <span class="math notranslate nohighlight">\(\sigma\)</span> controls the rate of decrease; for instance, when the distance is <span class="math notranslate nohighlight">\(\sigma\)</span>, the similarity is <span class="math notranslate nohighlight">\(e^{-1/2}\approx 0.6\)</span>.</p>
<p>There are ways to define similarity without making use of a distance, but we won’t be using them.</p>
<div class="section" id="distance-metrics">
<h2>Distance metrics<a class="headerlink" href="#distance-metrics" title="Permalink to this headline">¶</a></h2>
<div class="proof definition admonition" id="definition-similarity-metric">
<p class="admonition-title"><span class="caption-number">Definition 5.1.1 </span></p>
<div class="definition-content section" id="proof-content">
<p>A <strong>distance metric</strong> is a function dist on pairs of vectors that satisfies the following properties for all vectors:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\dist(\bfx,\bfy)=0\)</span> if and only if <span class="math notranslate nohighlight">\(\bfx=\bfy\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\dist(\bfx,\bfy) = \dist(\bfy,\bfx)\)</span>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(\dist(\bfx,\bfy) \le \dist(\bfx,\bfz) + \dist(\bfz,\bfy)\)</span>, known as the triangle inequality.</p></li>
</ol>
</div>
</div><p>These are considered the essential axioms of a distance metric. From them, you can also deduce that the distance function is always nonnegative. The term <em>distance metric</em> isn’t always used carefully to mean a function satisfying the three axioms, however, and some applications use a metric that does not satisfy the triangle inequality.</p>
<p>We already have the distance metric</p>
<div class="math notranslate nohighlight">
\[
\dist(\bfx,\bfy) = \norm{\bfx-\bfy}
\]</div>
<p>for any vector norm.</p>
<p>Another proper distance metric is <strong>angular distance</strong>. Generalizing from 2D and 3D vector geometry, we define the angle <span class="math notranslate nohighlight">\(\theta\)</span> between vectors <span class="math notranslate nohighlight">\(\bfu\)</span> and <span class="math notranslate nohighlight">\(\bfv\)</span> in <span class="math notranslate nohighlight">\(\real^d\)</span> by</p>
<div class="math notranslate nohighlight">
\[
\cos(\theta) = \frac{\mathbf{u}^T\mathbf{v}}{\twonorm{\mathbf{u}} \, \twonorm{\mathbf{v}}}.
\]</div>
<p>Then the quantity <span class="math notranslate nohighlight">\(\theta/\pi\)</span> is a distance metric. Because arccos is a relatively expensive computational operation, though, it’s common to use <strong>cosine similarity</strong>, defined as <span class="math notranslate nohighlight">\(\cos(\theta)\)</span>, and the related <strong>cosine distance</strong> <span class="math notranslate nohighlight">\(\tfrac{1}{2}[1-\cos(\theta)]\)</span>, even though the latter does not satisfy the triangle inequality.</p>
<p>Categorical variables can be included in distance measures. An ordinal variable is easily converted to equally spaced numerical values, which then may get a standard treatment. Nominal features are often compared using <strong>Hamming distance</strong>, which is just the total number of features that have different values in the two vectors.</p>
</div>
<div class="section" id="probability-distributions">
<h2>Probability distributions<a class="headerlink" href="#probability-distributions" title="Permalink to this headline">¶</a></h2>
<p>A <strong>discrete probability distribution</strong> is a vector <span class="math notranslate nohighlight">\(\bfx\)</span> whose components are nonnegative and satisfying <span class="math notranslate nohighlight">\(\onenorm{\bfx}=1\)</span>. Such a vector can be interpreted as frequencies or probabilities of observing different classes.</p>
<p>We already encountered one way to measure the dissimilarity of two probability distributions, the <em>cross-entropy</em>:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{CE}(\bfx,\bfy) = -\sum_{i=1}^d x_i \log(y_i). 
\]</div>
<p>A related measure is the <em>Kullback–Leibler diveregence</em>, or relative entropy,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{KL}(\bfx,\bfy) = \sum_{i=1}^d x_i \log\left( \frac{x_i}{y_i} \right).
\]</div>
<p>Whenever <span class="math notranslate nohighlight">\(0\cdot \log(0)\)</span> is encountered in the CE or KL definitions, it equals zero, in accordance with its limiting value from calculus.</p>
<p>Neither cross-entropy nor KL divergence are symmetric in their arguments. But there is a related true metric called <strong>information radius</strong>, defined as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{IR}(\bfu,\bfv) = \frac{1}{2} \left[ \operatorname{KL}(\bfu,\bfz) + \operatorname{KL}(\bfv,\bfz) \right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bfz=(\bfu+\bfv)/2\)</span>. Typically one uses a base-2 logarithm, in which case IR ranges between 0 and 1.</p>
<div class="proof example admonition" id="example-similarity-IR">
<p class="admonition-title"><span class="caption-number">Example 5.1.1 </span></p>
<div class="example-content section" id="proof-content">
<!-- Let $\bfu=[1,0]$ and $\bfv=[0,1]$. Then $\bfz=(\bfu+\bfv)/2=[\tfrac{1}{2},\tfrac{1}{2}]$, and

$$
\operatorname{KL}(\bfu,\bfz)  &= 1\cdot \log \left( \frac{1}{1/2} \right) + 0 \cdot \log \left( \frac{0}{1/2} \right) = 1,
\operatorname{KL}(\bfv,\bfz)  &= 0 \cdot \log \left( \frac{0}{1/2} \right) + 1 \cdot \log \left( \frac{1}{1/2} \right) = 1,
\operatorname{IR}(\bfu,\bfu)  &= \frac{1}{2} \left[ 1 + 1 \right] = 1. 
$$

Not surprisingly, these distribution vectors are as far apart as you can get.  -->
<p>Let <span class="math notranslate nohighlight">\(\bfu=\frac{1}{4}[1,3]\)</span> and <span class="math notranslate nohighlight">\(\bfv=\frac{1}{4}[3,1]\)</span>. Then <span class="math notranslate nohighlight">\(\bfz=(\bfu+\bfv)/2=[\tfrac{1}{2},\tfrac{1}{2}]\)</span>, and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{KL}(\bfu,\bfz)  &amp;= \tfrac{1}{4} \cdot \log \left( \frac{1/4}{1/2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3/4}{1/2} \right) \\ &amp;= \tfrac{1}{4} \cdot \log \left( \frac{1}{2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3}{2} \right) = -\tfrac{1}{4} + \tfrac{3}{4} (\log(3)-1),\\
\operatorname{KL}(\bfv,\bfz)  &amp;= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4},\\
\operatorname{IR}(\bfu,\bfu)  &amp;= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4} \approx 0.1887.
\end{split}\]</div>
</div>
</div></div>
<div class="section" id="distance-matrix">
<h2>Distance matrix<a class="headerlink" href="#distance-matrix" title="Permalink to this headline">¶</a></h2>
<p>Given the feature vectors <span class="math notranslate nohighlight">\(\bfx_1,\ldots,\bfx_n\)</span>, the pairwise distances between them can be summarized by the <span class="math notranslate nohighlight">\(n\times n\)</span> <strong>distance matrix</strong></p>
<div class="math notranslate nohighlight">
\[
D_{ij} = \text{dist}(\bfx_i,\bfx_j).
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(D_{ii}=0\)</span> and <span class="math notranslate nohighlight">\(D_{ji}=D_{ij}\)</span>. Many clustering algorithms allow supplying <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> in lieu of the feature vectors.</p>
<p>One can analogously define a <em>similarity matrix</em> using the Gaussian kernel. An advantage of similarity is that small values can be rounded down to zero. This has little effect on the results, but can create big gains in execution time and memory usage.</p>
<div class="section" id="toy-example">
<h3>Toy example<a class="headerlink" href="#toy-example" title="Permalink to this headline">¶</a></h3>
<p>Suppose we have sample points lying near the corners of a cube in 3D. This lets us compute an 8×8 distance matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">1.01</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">.98</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">1.02</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mf">.98</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">.99</span><span class="p">],[</span><span class="mf">1.01</span><span class="p">,</span><span class="mf">1.01</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
    <span class="p">[</span><span class="o">-</span><span class="mf">1.01</span><span class="p">,</span><span class="o">-</span><span class="mf">1.01</span><span class="p">,</span><span class="mf">.99</span><span class="p">],[</span><span class="mf">1.02</span><span class="p">,</span><span class="o">-</span><span class="mf">1.02</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mf">.99</span><span class="p">,</span><span class="mf">.98</span><span class="p">,</span><span class="mf">.99</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mf">.98</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> 
<span class="p">]</span>

<span class="n">D2</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">D2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/similarity_1_0.png" src="../_images/similarity_1_0.png" />
</div>
</div>
<p>Note the symmetry. Changing to a different norm is trivial.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D1</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;manhattan&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">D1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/similarity_3_0.png" src="../_images/similarity_3_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="distance-in-high-dimensions">
<h2>Distance in high dimensions<a class="headerlink" href="#distance-in-high-dimensions" title="Permalink to this headline">¶</a></h2>
<p>High-dimensional space <a class="reference external" href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">does not conform to some intuitions</a> formed by our experiences in 2D and 3D. For example, consider the unit hyperball <span class="math notranslate nohighlight">\(\twonorm{\bfx}\le 1\)</span> in <span class="math notranslate nohighlight">\(d\)</span> dimensions. We’ll take it as given that scaling a <span class="math notranslate nohighlight">\(d\)</span>-dimensional object by a number <span class="math notranslate nohighlight">\(r\)</span> will scale the volume by <span class="math notranslate nohighlight">\(r^d\)</span>. Then for any <span class="math notranslate nohighlight">\(r&lt;1\)</span>, the fraction of the unit hyperball’s volume lying <em>outside</em> the smaller hyperball of fixed radius <span class="math notranslate nohighlight">\(r\)</span> is <span class="math notranslate nohighlight">\(1-r^d\)</span>, which approaches <span class="math notranslate nohighlight">\(1\)</span> as <span class="math notranslate nohighlight">\(d\to \infty\)</span>. That is, if we choose points randomly within a hyperball, almost all of them will be near the outer boundary.</p>
<p>The volume of that unit hyperball also vanishes as <span class="math notranslate nohighlight">\(d\to \infty\)</span>. This is because the inequality</p>
<div class="math notranslate nohighlight">
\[
x_1^2 + x_2^2 + \cdots + x_d^2 \le 1,
\]</div>
<p>where each <span class="math notranslate nohighlight">\(x_i\)</span> is chosen randomly in <span class="math notranslate nohighlight">\([-1,1]\)</span>, becomes ever harder to satisfy as the number of terms in the sum grows, and the relative occurrence of such points is increasingly rare.</p>
<p>There are other, similar mathematical results demonstrating the unexpectedness of distances in high-dimensional space. These go under the colorful name <em>curse of dimensionality</em>, and the advice given in response to them is sometimes stated as, “Don’t use distance metrics in high-dimensional space.” But that advice is easy to overstate. The curse is essentially about <em>randomly</em> chosen points, and it is correct that dimensions of noisy or irrelevant features will make many learning algorithms less effective. But if features carry useful information, adding them usually makes matters better, not worse.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "UD-Math-Data-Science-1/notes",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./clustering"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="overview.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>Clustering</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="performance.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5.2. </span>Clustering performance</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tobin A. Driscoll<br/>
    
        &copy; Copyright 2022.<br/>
      <div class="extra_footer">
        <img alt='UD logo' src='_static/UDlogo-small.png'>
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>