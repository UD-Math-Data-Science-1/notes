{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96349ba",
   "metadata": {},
   "source": [
    "# Learning performance\n",
    "\n",
    "Good performance of a classifier on the training set is one thing, but how will it perform on new data? This is the question of *generalization*. In order to gauge this property, we will hold back some of the labeled data from training and use it solely to test the performance.\n",
    "\n",
    "We will continue demonstrating with the loan funding classification data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aed5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.loadtxt(\"data.csv\",delimiter=\",\")\n",
    "y = np.loadtxt(\"labels.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc1b79",
   "metadata": {},
   "source": [
    "We use a `sckikit` helper function to help us split off a randomized 20% of the data to use for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537a7d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312 training cases and 828 test cases\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2)\n",
    "print(len(y_tr),\"training cases and\",len(y_te),\"test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82272f29",
   "metadata": {},
   "source": [
    "Now we train on only the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e98b9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors as nbr\n",
    "knn = nbr.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9cc77",
   "metadata": {},
   "source": [
    "If we evaluate the performance on the training data, the classifier looks perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe6970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2693    0]\n",
      " [   0  619]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "yhat = knn.predict(X_tr)\n",
    "C = metrics.confusion_matrix(y_tr,yhat,labels=[-1,1])\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbeb7ad",
   "metadata": {},
   "source": [
    "But the picture is much different when we measure using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a3f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[562 122]\n",
      " [101  43]]\n"
     ]
    }
   ],
   "source": [
    "yhat = knn.predict(X_te)\n",
    "C = metrics.confusion_matrix(y_te,yhat,labels=[-1,1])\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc925e5f",
   "metadata": {},
   "source": [
    "We now see high false positive and false negative rates. This observation illustrates **overfitting**, which is the tendency of a model that learns too many idiosyncratic details about a training set to generalize well to new data.\n",
    "\n",
    "## Bias–variance tradeoff\n",
    "\n",
    "Suppose that $f(x)$ is a perfect labeling function over the entire population. Let $\\hat{f}(x)$ denote a particular labeling algorithm after training. Conceptually, $\\hat{f}$ is just one realization of all possible labelers that we might get from different training sets. Let $\\hat{y}$ denote the result of averaging all the labelers at $x$. Thus, there are two components to the performance of our labeler:\n",
    "\n",
    "* How well does $\\hat{y}$ approximate $f(x)$? This is the **bias** of the learner.\n",
    "* How close to $\\hat{y}$ is our $\\hat{f}(x)$ likely to be? This is the **variance** of the learner.\n",
    "\n",
    "There is a crude analogy with hitting the bullseye on a dartboard. A low-variance, high-bias learner will throw a tight cluster of darts far from the bullseye. A low-bias, high-variance learner will scatter the darts evenly all over the board.\n",
    "\n",
    "Most learning algorithms have one or more **hyperparameters** that are selected in advance by the designer rather than adjusted to fit the training data. Often, a hyperparameter can give the learner increased power in the form of additional degrees of freedom to use in fitting. Giving the learner more power might lead to decreased bias, because there is a larger universe of potential labelers to choose from. But it tends to increase variance, because the higher fidelity is actually used to fit more closely to the particular training set that is chosen. This dilemma is generally known as the **bias–variance tradeoff**.\n",
    "## Learning curves\n",
    "\n",
    "Let's illustrate overfitting with *decision trees*, a different type of classifier to be studied later. A decision tree has a controllable maximum depth which, when increased, allows it more freedom to fit training data.\n",
    "\n",
    "For this experiment, we vary $n$, the number of instances used to train the classifier. We first split off part of the data to serve as a test set. For each $n$, the rest of the data is randomly reordered before training, and we measure performance on the training set as well as the test set. As performance metric we use the `score` method of the classifier to get its accuracy, then subtract it from 1 to get an error measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798ebe78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cn/8w40979d27x0kv2wsnx7zgvm0000gp/T/ipykernel_98303/4209187316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtest_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m result = pd.DataFrame(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"train error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_err\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"size of training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2)\n",
    "N = range(50,3201,20)\n",
    "train_err = []\n",
    "test_err = []\n",
    "for n in N:\n",
    "  X_tr,y_tr = shuffle(X_tr,y_tr,random_state=1)\n",
    "  knn = tree.DecisionTreeClassifier(max_depth=5)   # specification\n",
    "  knn.fit(X_tr[:n,:],y_tr[:n])   # training\n",
    "  train_err.append(1-knn.score(X_tr[:n,:],y_tr[:n]))\n",
    "  test_err.append(1-knn.score(X_te,y_te))\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    {\"train error\":train_err,\"test error\":test_err},\n",
    "    index=pd.Series(N,name=\"size of training set\")\n",
    ")\n",
    "sns.lineplot(data=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d43b5",
   "metadata": {},
   "source": [
    "The plot above shows **learning curves**. Both curves converge to a horizontal asymptote. The gap between the curves is due to variance, which decreases as the training set grows. This is to be expected; generalizing from a few examples is probably harder than when many are available. The height of the curves is due to bias, which appears to be somewhere around a 16% error rate. This is a lower bound on the actual error, regardless of the training set; you can't knock out an elephant with a feather, no matter how many times you whack her with it.\n",
    "\n",
    "The curves above were for a tree depth of 5. The next plot shows it for a depth of 10, which increases the approximation power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5694f75c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cn/8w40979d27x0kv2wsnx7zgvm0000gp/T/ipykernel_98303/1984827150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtest_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m result = pd.DataFrame(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"train error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_err\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"size of training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2)\n",
    "N = range(50,3201,20)\n",
    "train_err = []\n",
    "test_err = []\n",
    "for n in N:\n",
    "  X_tr,y_tr = shuffle(X_tr,y_tr,random_state=1)\n",
    "  knn = tree.DecisionTreeClassifier(max_depth=10)   # specification\n",
    "  knn.fit(X_tr[:n,:],y_tr[:n])   # training\n",
    "  train_err.append(1-knn.score(X_tr[:n,:],y_tr[:n]))\n",
    "  test_err.append(1-knn.score(X_te,y_te))\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    {\"train error\":train_err,\"test error\":test_err},\n",
    "    index=pd.Series(N,name=\"size of training set\")\n",
    ")\n",
    "sns.lineplot(data=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61771ab8",
   "metadata": {},
   "source": [
    "This time, the curves do not come together before we run out of data. The nonzero variance suggests that the learner is in some sense overqualified to fit the available data. The bias did not get below 17% and appears to be levelling off, so that increased approximation power isn't even useful.\n",
    "\n",
    "Finally, we see what happens with a smaller depth of just 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabc3812",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cn/8w40979d27x0kv2wsnx7zgvm0000gp/T/ipykernel_98303/1984827150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtest_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m result = pd.DataFrame(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"train error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_err\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"size of training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2)\n",
    "N = range(50,3201,20)\n",
    "train_err = []\n",
    "test_err = []\n",
    "for n in N:\n",
    "  X_tr,y_tr = shuffle(X_tr,y_tr,random_state=1)\n",
    "  knn = tree.DecisionTreeClassifier(max_depth=10)   # specification\n",
    "  knn.fit(X_tr[:n,:],y_tr[:n])   # training\n",
    "  train_err.append(1-knn.score(X_tr[:n,:],y_tr[:n]))\n",
    "  test_err.append(1-knn.score(X_te,y_te))\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    {\"train error\":train_err,\"test error\":test_err},\n",
    "    index=pd.Series(N,name=\"size of training set\")\n",
    ")\n",
    "sns.lineplot(data=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83538d3",
   "metadata": {},
   "source": [
    "Now the variance is zero almost from the start. The bias is around 17%, so we have paid a small price for the lost power.\n",
    "\n",
    "The ideal bias–variance tradeoff for this classifier is probably a depth of 4 or 5. \n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "Now we can try different values for the parameter in the `knn` classifier, train, and test to find the best one. Here, we use accuracy as the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b3f024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7608695652173914, 0.8260869565217391, 0.7958937198067633, 0.8164251207729468, 0.8140096618357487, 0.821256038647343, 0.8236714975845411, 0.8260869565217391, 0.8236714975845411, 0.8272946859903382, 0.8248792270531401, 0.8285024154589372]\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "n = len(y_te)\n",
    "\n",
    "for k in range(1,13):\n",
    "    knn = nbr.KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_tr,y_tr)\n",
    "    yhat = knn.predict(X_te)\n",
    "    agree = sum(yhat==y_te)\n",
    "    acc.append(agree/n)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d39045",
   "metadata": {},
   "source": [
    "The experiment above suggests that we will not see much benefit from increasing the $k$ parameter past 6. This is known as **hyperparameter tuning**, because we are looking at the effect of varying a parameter that is not under the control of the learner algorithm.\n",
    "\n",
    "But we have created a new problem. If we optimize hyperparameters based on a performance metric over a fixed test set, then we have reintroduced the possibility of overfitting; i.e., the hyperparameters can be learned from the test set.\n",
    "\n",
    "There are two approaches to this dilemma. One is to split the data into *three* sets for training, testing, and an additional level of validation. However, this approach further reduces the amount of data available for training, which is likely to hurt performance.\n",
    "\n",
    "The other approach, called *cross-validation*, is to train the learner multiple times, each time using a different split of the data into training and testing. In **$k$-fold cross-validation**, the full data set is divided into $k$ roughly equal parts called *folds*. First, the learner is trained using folds $2,3,\\ldots,k$ and tested with the cases in fold 1. Then the learners are retrained using folds $1,3,\\ldots,k$ and tested with the cases in fold 2. This continues until each fold has served once as the test set.\n",
    "\n",
    "We demonstrate $k$-fold cross-validation for a particular KNN learner for $k=5$. By default, the performance metric will be the `knn.score` method, which is defined to compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ffd7987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81763285 0.80434783 0.8115942  0.82125604 0.80434783]\n",
      "mean: 0.8118357487922705 \n",
      "std:  0.0068490081539892686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = nbr.KNeighborsClassifier(n_neighbors=5)   # specification\n",
    "knn.fit(X,y)   # training\n",
    "scores = cross_val_score(knn,X,y,cv=5)\n",
    "\n",
    "print(scores)\n",
    "print(\"mean:\",scores.mean(),\"\\nstd: \",scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10def05c",
   "metadata": {},
   "source": [
    "The low variance of the scores among the different folds is reassurance that the measurements are based on representative test-train splits."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "source_map": [
   12,
   20,
   24,
   28,
   32,
   36,
   40,
   44,
   49,
   53,
   57,
   77,
   98,
   104,
   124,
   130,
   150,
   160,
   172,
   184,
   193
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}