{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b15fb5",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "The reinterpretation of classification methods as a form of regression on probability quickly leads to the question of looking for other ways to perform that regression. Specifically, can linear regression be adapted to that purpose? The answer is a qualified \"yes\".\n",
    "\n",
    "A linear regressor is the function $f(\\bfx) = \\bfx^T \\bfw$ for a constant vector $\\bfw$ (where we may augment $\\bfx$ with a constant in order to incorporate the intercept). It's not a good candidate for representing a probability, which should vary between 0 and 1. A simple remedy is to transform its output using the **logistic function**, which is defined as\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "\n",
    "```{figure} ../_static/logistic.png\n",
    "```\n",
    "\n",
    "The logistic function has the real line as its domain and takes the form of a smoothed step up from 0 to 1. Its inverse is the **logit function**,\n",
    "\n",
    "$$\n",
    "\\logit(p) = \\ln\\left( \\frac{p}{1-p} \\right).\n",
    "$$\n",
    "\n",
    "```{figure} ../_static/logit.png\n",
    "```\n",
    "\n",
    "When interpreting $p$ as probability, $\\logit(p)$ is the **log-odds ratio**. For instance, if $p=2/3$, then the odds ratio is $(2/3)/(1/3)=2$ (i.e., 2:1 odds), and $\\logit(2/3)=\\ln(2)$. \n",
    "\n",
    "A natural use of linear regression, which has a range over all real numbers, is to match it to the logit of probability, rather than to probability itself:\n",
    "\n",
    "$$\n",
    "\\logit(p) \\approx \\bfx^T\\bfw.\n",
    "$$\n",
    "\n",
    "This implies multilinear regression for the function $\\logit(p)$, where $p$ is the probability of the class $y=1$. Equivalently,\n",
    "\n",
    "$$\n",
    "p \\approx \\sigma(\\bfx^T\\bfw).\n",
    "$$\n",
    "\n",
    "The resulting method is called **logistic regression**.\n",
    "\n",
    "## Loss function\n",
    "\n",
    "At each training observation $(\\bfx_i,y_i)$, we know that either $y_i=0$ or $y_i=1$. Extending the loss function for linear regression to the logistic case would suggest the minimization of least squares,\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\left[ \\bfx_i^T\\bfw - \\logit(y_i) \\right]^2. \n",
    "$$\n",
    "\n",
    "However, the logits in this expression are all infinite, so a different loss function must be identified. One possibility is \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\left[ \\hat{p}_i - y_i \\right]^2, \\qquad \\hat{p}_i = \\sigma(\\bfx_i^T\\bfw) .\n",
    "$$\n",
    "\n",
    "It's more common to minimize the **cross-entropy** loss function\n",
    "\n",
    "$$\n",
    "L(\\bfw) = -\\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i) \\right].\n",
    "$$\n",
    "\n",
    "The logarithms above can have any base, since that choice only changes $L$ by a constant factor. Note that in cross-entropy, observation $i$ contributes $-\\log(1-\\hat{p}_i)$ if $y_i=0$ and $-\\log(\\hat{p}_i)$ if $y_i=1$. This loss function creates an unboundedly large penalty as $\\hat{p}_i \\to 1$ if $y_i=0$, and vice versa, which often makes it preferable to the least-squares alternative above.\n",
    "\n",
    "Logistic regression does have a major disadvantage compared to (multi)linear regression: the minimization of loss does *not* lead to a linear problem for the weight vector $\\bfw$. The difference in practice is usually not concerning, though.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "As with other forms of regression, the loss function may be regularized using the ridge or LASSO penalty. The standard formulation is \n",
    "\n",
    "$$\n",
    "\\widetilde{L}(\\bfw) = C \\, L(\\bfw) + \\norm{\\bfw},\n",
    "$$\n",
    "\n",
    "where $C$ is a positive hyperparameter and the vector norm is either the 2-norm (ridge) or 1-norm (LASSO). Note that $C$ functions like the inverse of the regularization parameter $\\alpha$ in our linear regressor. This is simply a different convention (like the one for the SVM), but it means that smaller values of $C$ imply *greater* amounts of regularization.\n",
    "\n",
    "## Case study: Personal spam filter\n",
    "\n",
    "We will try logistic regression for a simple spam filter. The data set is based on work and personal emails for one individual. The features are calculated word and character frequencies, as well as the appearance of capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114d4453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_%3B</th>\n",
       "      <th>char_freq_%28</th>\n",
       "      <th>char_freq_%5B</th>\n",
       "      <th>char_freq_%21</th>\n",
       "      <th>char_freq_%24</th>\n",
       "      <th>char_freq_%23</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0               0.00               0.64           0.64           0.0   \n",
       "1               0.21               0.28           0.50           0.0   \n",
       "2               0.06               0.00           0.71           0.0   \n",
       "3               0.00               0.00           0.00           0.0   \n",
       "4               0.00               0.00           0.00           0.0   \n",
       "...              ...                ...            ...           ...   \n",
       "4596            0.31               0.00           0.62           0.0   \n",
       "4597            0.00               0.00           0.00           0.0   \n",
       "4598            0.30               0.00           0.30           0.0   \n",
       "4599            0.96               0.00           0.00           0.0   \n",
       "4600            0.00               0.00           0.65           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0              0.32            0.00              0.00                0.00   \n",
       "1              0.14            0.28              0.21                0.07   \n",
       "2              1.23            0.19              0.19                0.12   \n",
       "3              0.63            0.00              0.31                0.63   \n",
       "4              0.63            0.00              0.31                0.63   \n",
       "...             ...             ...               ...                 ...   \n",
       "4596           0.00            0.31              0.00                0.00   \n",
       "4597           0.00            0.00              0.00                0.00   \n",
       "4598           0.00            0.00              0.00                0.00   \n",
       "4599           0.32            0.00              0.00                0.00   \n",
       "4600           0.00            0.00              0.00                0.00   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  char_freq_%3B  char_freq_%28  \\\n",
       "0                0.00            0.00  ...          0.000          0.000   \n",
       "1                0.00            0.94  ...          0.000          0.132   \n",
       "2                0.64            0.25  ...          0.010          0.143   \n",
       "3                0.31            0.63  ...          0.000          0.137   \n",
       "4                0.31            0.63  ...          0.000          0.135   \n",
       "...               ...             ...  ...            ...            ...   \n",
       "4596             0.00            0.00  ...          0.000          0.232   \n",
       "4597             0.00            0.00  ...          0.000          0.000   \n",
       "4598             0.00            0.00  ...          0.102          0.718   \n",
       "4599             0.00            0.00  ...          0.000          0.057   \n",
       "4600             0.00            0.00  ...          0.000          0.000   \n",
       "\n",
       "      char_freq_%5B  char_freq_%21  char_freq_%24  char_freq_%23  \\\n",
       "0               0.0          0.778          0.000          0.000   \n",
       "1               0.0          0.372          0.180          0.048   \n",
       "2               0.0          0.276          0.184          0.010   \n",
       "3               0.0          0.137          0.000          0.000   \n",
       "4               0.0          0.135          0.000          0.000   \n",
       "...             ...            ...            ...            ...   \n",
       "4596            0.0          0.000          0.000          0.000   \n",
       "4597            0.0          0.353          0.000          0.000   \n",
       "4598            0.0          0.000          0.000          0.000   \n",
       "4599            0.0          0.000          0.000          0.000   \n",
       "4600            0.0          0.125          0.000          0.000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                          3.756                          61   \n",
       "1                          5.114                         101   \n",
       "2                          9.821                         485   \n",
       "3                          3.537                          40   \n",
       "4                          3.537                          40   \n",
       "...                          ...                         ...   \n",
       "4596                       1.142                           3   \n",
       "4597                       1.555                           4   \n",
       "4598                       1.404                           6   \n",
       "4599                       1.147                           5   \n",
       "4600                       1.250                           5   \n",
       "\n",
       "      capital_run_length_total  class  \n",
       "0                          278      1  \n",
       "1                         1028      1  \n",
       "2                         2259      1  \n",
       "3                          191      1  \n",
       "4                          191      1  \n",
       "...                        ...    ...  \n",
       "4596                        88      0  \n",
       "4597                        14      0  \n",
       "4598                       118      0  \n",
       "4599                        78      0  \n",
       "4600                        40      0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spam = pd.read_csv(\"spambase.csv\")\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a64024",
   "metadata": {},
   "source": [
    "We'll create a feature matrix and label vector, and split into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291d3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam.drop(\"class\",axis=\"columns\")\n",
    "y = spam[\"class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.2,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cbbc1a",
   "metadata": {},
   "source": [
    "When using norm-based regularization, it's good practice to standardize the variables, so we will prepare to set up a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b09d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923f73c",
   "metadata": {},
   "source": [
    "First we use a large value of $C$ to emphasize the regressive loss rather than the regularization penalty. (The default regularization norm is the 2-norm.) It's not required to select a solver, but we choose one here that is reliable for small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28758e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9337676438653637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression(C=100,solver=\"liblinear\")\n",
    "pipe = make_pipeline(StandardScaler(),logr)\n",
    "pipe.fit(X_tr,y_tr)\n",
    "print(\"accuracy:\",pipe.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40251c03",
   "metadata": {},
   "source": [
    "Let's look at the most extreme regression coefficients, associating them with the feature names and then sorting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0e0fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least spammy:\n",
      "word_freq_george    -24.055873\n",
      "word_freq_cs         -8.573934\n",
      "word_freq_hp         -3.512677\n",
      "word_freq_meeting    -1.940974\n",
      "dtype: float64\n",
      "\n",
      "most spammy:\n",
      "char_freq_%23                 1.155145\n",
      "char_freq_%24                 1.262550\n",
      "capital_run_length_longest    1.975006\n",
      "word_freq_3d                  2.112929\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coef = pd.Series(logr.coef_[0],index=X.columns).sort_values()\n",
    "print(\"least spammy:\")\n",
    "print(coef[:4])\n",
    "print(\"\\nmost spammy:\")\n",
    "print(coef[-4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ea2eb",
   "metadata": {},
   "source": [
    "The word \"george\" is a strong counter-indicator for spam (remember that this data set comes from an individual). Its presence makes the inner product $\\bfx^T\\bfw$ more negative, which drives the logistic function closer to 0. Conversely, the presence of consecutive capital letters increases the inner product and pushes the probability closer to 1. \n",
    "\n",
    "The predictions by the regressor are all either 0 or 1. But we can also see the forecasted probabilities before thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea94703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted classes:\n",
      "[0 0 0 0 0]\n",
      "\n",
      "probabilities:\n",
      "[[0.53769264 0.46230736]\n",
      " [0.99694715 0.00305285]\n",
      " [0.63975661 0.36024339]\n",
      " [0.996342   0.003658  ]\n",
      " [0.93740435 0.06259565]]\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted classes:\")\n",
    "print(pipe.predict(X_tr.iloc[:5,:]))\n",
    "print(\"\\nprobabilities:\")\n",
    "print(pipe.predict_proba(X_tr.iloc[:5,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775b0f4",
   "metadata": {},
   "source": [
    "The probabilities might be useful for making decisions based on the results. For example, the first instance above was much less certain about the classification than the second, and a lower threshold for determining spam might have changed the class to 1. The probability matrix can be used to create an ROC curve showing the tradeoffs over all thresholds.\n",
    "\n",
    "For a validation-based selection of the best regularization parameter value, we can use `LogisticRegressionCV`, which is basically a convenience method for a grid search. You can specify which values of $C$ to search over, or just say how many, as we do here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e086d18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C value: 21.5\n",
      "accuracy score: 0.9349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logr = LogisticRegressionCV(Cs=40,cv=5,solver=\"liblinear\",random_state=0)\n",
    "pipe = make_pipeline(StandardScaler(),logr)\n",
    "pipe.fit(X_tr,y_tr)\n",
    "\n",
    "print(f\"best C value: {logr.C_[0]:.3g}\")\n",
    "print(f\"accuracy score: {pipe.score(X_te,y_te):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d02f78",
   "metadata": {},
   "source": [
    "## Multiclass case\n",
    "\n",
    "When there are more than two unique labels possible, logistic regression can be extended through the **one-vs-rest** (OVR) paradigm. Given $K$ classes, there are $K$ binary regressors fit for the outcomes \"class 1/not class 1,\" \"class 2/not class 2,\" and so on, giving $K$ different coefficient vectors, $\\bfw_k$. Now for a sample point $\\bfx_i$ we predict probabilities for it being in each class:\n",
    "\n",
    "$$\n",
    "\\hat{q}_{i,k} = \\sigma(\\bfx_i^T \\bfw_k), \\qquad k=1,\\ldots,K. \n",
    "$$\n",
    "\n",
    "Since the $K$ OVR regressors are done independently, there is no reason to think these probabilities will sum to 1 over all the classes. But it's easy to normalize them:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{i,k} = \\frac{\\hat{q}_{i,k}}{\\sum_{k=1}^K \\hat{q}_{i,k}}.\n",
    "$$\n",
    "\n",
    "That is, we get a matrix of probabilities. Each of the $n$ rows gives the class probabilities at a single sample point, and each of the $K$ columns gives the probability of one class at all the samples.\n",
    "\n",
    "\n",
    "## Case study: Gas sensor drift\n",
    "\n",
    "As a multiclass example, we use a data set about gas sensors recording values over long periods of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201c27f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.98705966930266\n"
     ]
    }
   ],
   "source": [
    "gas = pd.read_csv(\"gas_drift.csv\")\n",
    "y = gas[\"Class\"]\n",
    "X = gas.drop(\"Class\",axis=\"columns\")\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.2,shuffle=True,random_state=1)\n",
    "\n",
    "logr = LogisticRegression(solver=\"liblinear\")\n",
    "pipe = make_pipeline(StandardScaler(),logr)\n",
    "pipe.fit(X_tr,y_tr)\n",
    "print(\"accuracy score:\",pipe.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34627d55",
   "metadata": {},
   "source": [
    "We can now look at predictions of probability for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b3e413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class 1</th>\n",
       "      <th>Class 2</th>\n",
       "      <th>Class 3</th>\n",
       "      <th>Class 4</th>\n",
       "      <th>Class 5</th>\n",
       "      <th>Class 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>5.335916e-06</td>\n",
       "      <td>0.020479</td>\n",
       "      <td>4.207813e-03</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>9.719636e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.999199e-01</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>6.812814e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.883007e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008648</td>\n",
       "      <td>3.856552e-03</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>1.282034e-03</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>9.849029e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.237644</td>\n",
       "      <td>5.483765e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>7.221450e-01</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>4.013725e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016620</td>\n",
       "      <td>3.644173e-02</td>\n",
       "      <td>0.010126</td>\n",
       "      <td>2.032038e-01</td>\n",
       "      <td>0.032094</td>\n",
       "      <td>7.015147e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.045888e-01</td>\n",
       "      <td>0.060927</td>\n",
       "      <td>2.871603e-03</td>\n",
       "      <td>0.031522</td>\n",
       "      <td>8.949040e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>1.782929e-04</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>7.461637e-04</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>9.794063e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>0.000254</td>\n",
       "      <td>2.038618e-05</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>2.361770e-04</td>\n",
       "      <td>0.024404</td>\n",
       "      <td>9.706624e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2780</th>\n",
       "      <td>0.009284</td>\n",
       "      <td>4.194312e-08</td>\n",
       "      <td>0.989908</td>\n",
       "      <td>6.143468e-04</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1.676350e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2781</th>\n",
       "      <td>0.004260</td>\n",
       "      <td>3.264299e-02</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>9.340926e-01</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>7.467973e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2782 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Class 1       Class 2   Class 3       Class 4   Class 5       Class 6\n",
       "0     0.000153  5.335916e-06  0.020479  4.207813e-03  0.003191  9.719636e-01\n",
       "1     0.000004  9.999199e-01  0.000074  6.812814e-07  0.000002  5.883007e-17\n",
       "2     0.008648  3.856552e-03  0.000017  1.282034e-03  0.001294  9.849029e-01\n",
       "3     0.237644  5.483765e-08  0.000020  7.221450e-01  0.000054  4.013725e-02\n",
       "4     0.016620  3.644173e-02  0.010126  2.032038e-01  0.032094  7.015147e-01\n",
       "...        ...           ...       ...           ...       ...           ...\n",
       "2777  0.000002  9.045888e-01  0.060927  2.871603e-03  0.031522  8.949040e-05\n",
       "2778  0.000089  1.782929e-04  0.001882  7.461637e-04  0.017699  9.794063e-01\n",
       "2779  0.000254  2.038618e-05  0.004423  2.361770e-04  0.024404  9.706624e-01\n",
       "2780  0.009284  4.194312e-08  0.989908  6.143468e-04  0.000025  1.676350e-04\n",
       "2781  0.004260  3.264299e-02  0.007537  9.340926e-01  0.013999  7.467973e-03\n",
       "\n",
       "[2782 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "p_hat = pipe.predict_proba(X_te)\n",
    "results = pd.DataFrame(p_hat,columns=[\"Class \"+str(i) for i in range(1,7)])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f9434",
   "metadata": {},
   "source": [
    "Here is a look at how the maximum prediction probability for each row in the test set is distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145d2cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/driscoll/Dropbox/class/267/notes/_build/jupyter_execute/regression/logistic_19_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.displot(x=np.max(p_hat,axis=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3351f9",
   "metadata": {},
   "source": [
    "You can see from the plot that a solid majority of classifications are made with at least 90% probability. So if we set a high threshold for classification, we should get few false positives while still getting good recall. An AUC-ROC score can be computed by averaging the values over the curves for each class. In this case, AUC-ROC score is very high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a79960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985961583670022"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_te,p_hat,multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18be948",
   "metadata": {},
   "source": [
    "<div style=\"max-width:608px\"><div style=\"position:relative;padding-bottom:66.118421052632%\"><iframe id=\"kaltura_player\" src=\"https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_mr2gh70i&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_en335xwu\" width=\"608\" height=\"402\" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow=\"autoplay *; fullscreen *; encrypted-media *\" sandbox=\"allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation\" frameborder=\"0\" title=\"Kaltura Player\" style=\"position:absolute;top:0;left:0;width:100%;height:100%\"></iframe></div></div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "source_map": [
   12,
   90,
   94,
   98,
   104,
   108,
   111,
   115,
   121,
   125,
   131,
   137,
   142,
   148,
   156,
   179,
   189,
   193,
   198,
   202,
   207,
   211,
   214
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}