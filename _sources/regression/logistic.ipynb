{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0590c3d",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression is, somewhat paradoxically, most often used for classification rather than a regression problem. In the case of **binary logistic regression**, the labels of each instance is either 0 or 1, but the regressor predicts a real number between zero and one. This value is typically interpreted as the probability of observing a 1, and then a threshold is chosen to quantize the output to 0 or 1.\n",
    "\n",
    "## Logistic and logit functions\n",
    "\n",
    "The **logistic function** is defined as\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "\n",
    "```{figure} ../_static/logistic.png\n",
    "```\n",
    "\n",
    "The logistic function takes the form of a smoothed step up from 0 to 1. Its inverse is the **logit function**,\n",
    "\n",
    "$$\n",
    "\\logit(p) = \\ln\\left( \\frac{p}{1-p} \\right).\n",
    "$$\n",
    "\n",
    "```{figure} ../_static/logit.png\n",
    "```\n",
    "\n",
    "In keeping with interpreting $p$ as probability, $\\logit(p)$ is the **log-odds ratio**. For instance, if $p=2/3$, then the odds ratio is $(2/3)/(1/3)=2$ (i.e., 2:1 odds), and $\\logit(2/3)=\\ln(2)$. \n",
    "\n",
    "Logistic regression is the approximation\n",
    "\n",
    "$$\n",
    "\\logit(p) \\approx \\bfx^T\\bfw,\n",
    "$$\n",
    "\n",
    "that is, multilinear regression for the function $\\logit(p)$, where $p$ is the probability of the class $y=1$. Hence\n",
    "\n",
    "$$\n",
    "p \\approx \\sigma(\\bfx^T\\bfw).\n",
    "$$\n",
    "\n",
    "## Loss function\n",
    "\n",
    "At a training observation $(\\bfx_i,y_i)$, we know that either $p=0$ or $p=1$. Let $\\hat{p}_i$ be the output of the regressor at this observation:\n",
    "\n",
    "$$\n",
    "\\hat{p}_i  = \\sigma(\\bfx_i^T\\bfw).\n",
    "$$\n",
    "\n",
    "The loss function is then\n",
    "\n",
    "$$\n",
    "L(\\bfw) = -\\sum_{i=1}^n \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i) \\right].\n",
    "$$\n",
    "\n",
    "Note that observation $i$ contributes $-\\ln(1-\\hat{p}_i)$ if $y_i=0$ and $-\\ln(\\hat{p}_i)$ if $y_i=1$. Both quantities increase as $\\hat{p}_i$ gets farther away from $y_i$. This loss is a special case of **cross-entropy**, a measure of dissimilarity between the probabilities of 1 occurring in the training versus the prediction.\n",
    "\n",
    "As with other forms of linear regression, the loss function is often regularized using the ridge or LASSO penalty. As we covered earlier, there is a hyperparameter $C$ that emphasizes small $\\norm{\\bfw}$ as $C\\to 0$, and pure regression as $C\\to \\infty$. \n",
    "\n",
    "\n",
    "## Case study: Personal spam filter\n",
    "\n",
    "We will try logistic regression for a simple spam filter. The data set is based on work and personal emails for one individual. The features are calculated word and character frequencies, as well as the appearance of capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145914c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_%3B</th>\n",
       "      <th>char_freq_%28</th>\n",
       "      <th>char_freq_%5B</th>\n",
       "      <th>char_freq_%21</th>\n",
       "      <th>char_freq_%24</th>\n",
       "      <th>char_freq_%23</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0               0.00               0.64           0.64           0.0   \n",
       "1               0.21               0.28           0.50           0.0   \n",
       "2               0.06               0.00           0.71           0.0   \n",
       "3               0.00               0.00           0.00           0.0   \n",
       "4               0.00               0.00           0.00           0.0   \n",
       "...              ...                ...            ...           ...   \n",
       "4596            0.31               0.00           0.62           0.0   \n",
       "4597            0.00               0.00           0.00           0.0   \n",
       "4598            0.30               0.00           0.30           0.0   \n",
       "4599            0.96               0.00           0.00           0.0   \n",
       "4600            0.00               0.00           0.65           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0              0.32            0.00              0.00                0.00   \n",
       "1              0.14            0.28              0.21                0.07   \n",
       "2              1.23            0.19              0.19                0.12   \n",
       "3              0.63            0.00              0.31                0.63   \n",
       "4              0.63            0.00              0.31                0.63   \n",
       "...             ...             ...               ...                 ...   \n",
       "4596           0.00            0.31              0.00                0.00   \n",
       "4597           0.00            0.00              0.00                0.00   \n",
       "4598           0.00            0.00              0.00                0.00   \n",
       "4599           0.32            0.00              0.00                0.00   \n",
       "4600           0.00            0.00              0.00                0.00   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  char_freq_%3B  char_freq_%28  \\\n",
       "0                0.00            0.00  ...          0.000          0.000   \n",
       "1                0.00            0.94  ...          0.000          0.132   \n",
       "2                0.64            0.25  ...          0.010          0.143   \n",
       "3                0.31            0.63  ...          0.000          0.137   \n",
       "4                0.31            0.63  ...          0.000          0.135   \n",
       "...               ...             ...  ...            ...            ...   \n",
       "4596             0.00            0.00  ...          0.000          0.232   \n",
       "4597             0.00            0.00  ...          0.000          0.000   \n",
       "4598             0.00            0.00  ...          0.102          0.718   \n",
       "4599             0.00            0.00  ...          0.000          0.057   \n",
       "4600             0.00            0.00  ...          0.000          0.000   \n",
       "\n",
       "      char_freq_%5B  char_freq_%21  char_freq_%24  char_freq_%23  \\\n",
       "0               0.0          0.778          0.000          0.000   \n",
       "1               0.0          0.372          0.180          0.048   \n",
       "2               0.0          0.276          0.184          0.010   \n",
       "3               0.0          0.137          0.000          0.000   \n",
       "4               0.0          0.135          0.000          0.000   \n",
       "...             ...            ...            ...            ...   \n",
       "4596            0.0          0.000          0.000          0.000   \n",
       "4597            0.0          0.353          0.000          0.000   \n",
       "4598            0.0          0.000          0.000          0.000   \n",
       "4599            0.0          0.000          0.000          0.000   \n",
       "4600            0.0          0.125          0.000          0.000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                          3.756                          61   \n",
       "1                          5.114                         101   \n",
       "2                          9.821                         485   \n",
       "3                          3.537                          40   \n",
       "4                          3.537                          40   \n",
       "...                          ...                         ...   \n",
       "4596                       1.142                           3   \n",
       "4597                       1.555                           4   \n",
       "4598                       1.404                           6   \n",
       "4599                       1.147                           5   \n",
       "4600                       1.250                           5   \n",
       "\n",
       "      capital_run_length_total  class  \n",
       "0                          278      1  \n",
       "1                         1028      1  \n",
       "2                         2259      1  \n",
       "3                          191      1  \n",
       "4                          191      1  \n",
       "...                        ...    ...  \n",
       "4596                        88      0  \n",
       "4597                        14      0  \n",
       "4598                       118      0  \n",
       "4599                        78      0  \n",
       "4600                        40      0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spam = pd.read_csv(\"spambase.csv\")\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b36bfb",
   "metadata": {},
   "source": [
    "We'll create a feature matrix and label vector, and split into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cd2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam.drop(\"class\",axis=\"columns\")\n",
    "y = spam[\"class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.2,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4fe0bb",
   "metadata": {},
   "source": [
    "When using norm-based regularization, it's good practice to standardize the variables, so we will prepare to set up a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810d11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b56b87",
   "metadata": {},
   "source": [
    "First we use a large value of $C$ to emphasize the regressive loss rather than the regularization penalty. (The default regularization norm is the 2-norm.) It's not required to select a solver, but we choose one here that is reliable for small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29213995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9337676438653637"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression(C=100,solver=\"liblinear\")\n",
    "pipe = make_pipeline(StandardScaler(),logr)\n",
    "pipe.fit(X_tr,y_tr)\n",
    "pipe.score(X_te,y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34175c03",
   "metadata": {},
   "source": [
    "Let's look at the most extreme regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6696490b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_george             -24.055873\n",
       "word_freq_cs                  -8.573934\n",
       "word_freq_hp                  -3.512677\n",
       "word_freq_meeting             -1.940974\n",
       "word_freq_lab                 -1.469316\n",
       "word_freq_edu                 -1.460705\n",
       "word_freq_hpl                 -1.181911\n",
       "word_freq_85                  -1.177344\n",
       "word_freq_re                  -0.981293\n",
       "word_freq_conference          -0.934152\n",
       "word_freq_project             -0.776980\n",
       "word_freq_pm                  -0.477826\n",
       "char_freq_%3B                 -0.379451\n",
       "capital_run_length_average    -0.367801\n",
       "word_freq_data                -0.346597\n",
       "word_freq_labs                -0.287406\n",
       "word_freq_original            -0.271247\n",
       "char_freq_%5B                 -0.209782\n",
       "word_freq_address             -0.196341\n",
       "word_freq_table               -0.183580\n",
       "word_freq_parts               -0.165103\n",
       "word_freq_direct              -0.135774\n",
       "word_freq_will                -0.132897\n",
       "word_freq_1999                -0.124816\n",
       "word_freq_make                -0.110128\n",
       "word_freq_receive             -0.082429\n",
       "char_freq_%28                 -0.067925\n",
       "word_freq_people              -0.021478\n",
       "word_freq_email               -0.004437\n",
       "word_freq_report               0.022125\n",
       "word_freq_all                  0.058893\n",
       "word_freq_mail                 0.076243\n",
       "word_freq_money                0.149010\n",
       "word_freq_you                  0.162890\n",
       "word_freq_order                0.183803\n",
       "word_freq_telnet               0.212813\n",
       "word_freq_over                 0.221469\n",
       "word_freq_650                  0.230857\n",
       "word_freq_font                 0.239646\n",
       "word_freq_your                 0.249595\n",
       "word_freq_415                  0.249837\n",
       "char_freq_%21                  0.250076\n",
       "word_freq_internet             0.265411\n",
       "word_freq_857                  0.281856\n",
       "word_freq_our                  0.295921\n",
       "word_freq_business             0.396552\n",
       "word_freq_technology           0.417834\n",
       "capital_run_length_total       0.428443\n",
       "word_freq_credit               0.458949\n",
       "word_freq_addresses            0.504272\n",
       "word_freq_000                  0.730850\n",
       "word_freq_remove               0.802990\n",
       "word_freq_free                 1.135052\n",
       "char_freq_%23                  1.155145\n",
       "char_freq_%24                  1.262550\n",
       "capital_run_length_longest     1.975006\n",
       "word_freq_3d                   2.112929\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(logr.coef_[0],index=X.columns).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43421d",
   "metadata": {},
   "source": [
    "The word \"george\" is a strong counter-indicator for spam (remember that this data set comes from an individual), while the presence of \"free\" or consecutive capital letters is a strong signal of spam. \n",
    "\n",
    "The predictions by the regressor are all either 0 or 1. But we can also see the forecasted probabilities before rounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f999dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:\n",
      "[0 0 0 0 0]\n",
      "\n",
      "probabilities:\n",
      "[[0.53769264 0.46230736]\n",
      " [0.99694715 0.00305285]\n",
      " [0.63975661 0.36024339]\n",
      " [0.996342   0.003658  ]\n",
      " [0.93740435 0.06259565]]\n"
     ]
    }
   ],
   "source": [
    "print(\"classes:\")\n",
    "print(pipe.predict(X_tr.iloc[:5,:]))\n",
    "print(\"\\nprobabilities:\")\n",
    "print(pipe.predict_proba(X_tr.iloc[:5,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d0f32",
   "metadata": {},
   "source": [
    "The probabilities might be useful, e.g., to make decisions based on the results.\n",
    "\n",
    "For a validation-based selection of the best regularization parameter, we can use `LogisticRegressionCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27593a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C value: 21.5\n",
      "R2 score: 0.9349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logr = LogisticRegressionCV(Cs=40,cv=5,solver=\"liblinear\")\n",
    "pipe = make_pipeline(StandardScaler(),logr)\n",
    "pipe.fit(X_tr,y_tr)\n",
    "\n",
    "print(f\"best C value: {logr.C_[0]:.3g}\")\n",
    "print(f\"R2 score: {pipe.score(X_te,y_te):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160feee3",
   "metadata": {},
   "source": [
    "## Multiclass case\n",
    "\n",
    "When there are more than two unique labels possible, logistic regression can be extended through the **one-vs-rest** paradigm. Given $K$ classes, there are $K$ binary regressors fit for the outcomes \"class 1/not class 1,\" \"class 2/not class 2,\" and so on. This gives predictive relative probabilities $q_1,\\ldots,q_K$ for the occurrence of each individual class. Since they need not sum to 1, they can be normalized into predicted probabilities via\n",
    "\n",
    "$$\n",
    "\\hat{p}_i = \\frac{q_i}{\\sum_{k=1}^K q_k}.\n",
    "$$\n",
    "\n",
    "<!-- \n",
    "Another way to convert them is by using a **softmax** function:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{q_i}}{\\sum_{k=1}^K e^{q_k}}.\n",
    "$$\n",
    "\n",
    "The softmax exaggerates differences between the $q_i$, making the result closer to a \"winner takes all\" result.\n",
    " -->\n",
    "\n",
    "## Case study: Gas sensor drift\n",
    "\n",
    "As a multiclass example, we use a data set about gas sensors recording values over long periods of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2d271b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98705966930266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas = pd.read_csv(\"gas_drift.csv\")\n",
    "y = gas[\"Class\"]\n",
    "X = gas.drop(\"Class\",axis=\"columns\")\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.2,shuffle=True,random_state=1)\n",
    "\n",
    "logr = LogisticRegression(solver=\"liblinear\")\n",
    "pipe = make_pipeline(StandardScaler(),logr)\n",
    "pipe.fit(X_tr,y_tr)\n",
    "pipe.score(X_te,y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce0caa",
   "metadata": {},
   "source": [
    "We can now look at predictions of probability for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0892dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class 1</th>\n",
       "      <th>Class 2</th>\n",
       "      <th>Class 3</th>\n",
       "      <th>Class 4</th>\n",
       "      <th>Class 5</th>\n",
       "      <th>Class 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063799</td>\n",
       "      <td>7.519237e-03</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.571542</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>3.469312e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006095</td>\n",
       "      <td>5.080764e-01</td>\n",
       "      <td>0.023668</td>\n",
       "      <td>0.407596</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>1.058263e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061460</td>\n",
       "      <td>6.384422e-08</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.937864</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>4.848255e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000550</td>\n",
       "      <td>1.155326e-05</td>\n",
       "      <td>0.382025</td>\n",
       "      <td>0.532771</td>\n",
       "      <td>0.084641</td>\n",
       "      <td>8.565403e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034819</td>\n",
       "      <td>1.014740e-05</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.959959</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>3.567080e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13905</th>\n",
       "      <td>0.005055</td>\n",
       "      <td>5.448815e-03</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.052044</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>9.343653e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13906</th>\n",
       "      <td>0.008545</td>\n",
       "      <td>7.510422e-03</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.039269</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>9.427741e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13907</th>\n",
       "      <td>0.007100</td>\n",
       "      <td>6.657342e-03</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.040465</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>9.430110e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13908</th>\n",
       "      <td>0.004199</td>\n",
       "      <td>6.163895e-03</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.057378</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>9.285169e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13909</th>\n",
       "      <td>0.006764</td>\n",
       "      <td>1.256542e-02</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>9.608898e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13910 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class 1       Class 2   Class 3   Class 4   Class 5       Class 6\n",
       "0      0.063799  7.519237e-03  0.001233  0.571542  0.008976  3.469312e-01\n",
       "1      0.006095  5.080764e-01  0.023668  0.407596  0.043982  1.058263e-02\n",
       "2      0.061460  6.384422e-08  0.000002  0.937864  0.000190  4.848255e-04\n",
       "3      0.000550  1.155326e-05  0.382025  0.532771  0.084641  8.565403e-11\n",
       "4      0.034819  1.014740e-05  0.000034  0.959959  0.001611  3.567080e-03\n",
       "...         ...           ...       ...       ...       ...           ...\n",
       "13905  0.005055  5.448815e-03  0.001287  0.052044  0.001800  9.343653e-01\n",
       "13906  0.008545  7.510422e-03  0.000853  0.039269  0.001048  9.427741e-01\n",
       "13907  0.007100  6.657342e-03  0.000892  0.040465  0.001874  9.430110e-01\n",
       "13908  0.004199  6.163895e-03  0.002197  0.057378  0.001545  9.285169e-01\n",
       "13909  0.006764  1.256542e-02  0.001300  0.015904  0.002576  9.608898e-01\n",
       "\n",
       "[13910 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "phat = pipe.predict_proba(X)\n",
    "pd.DataFrame(phat,columns=[\"Class \"+str(i) for i in range(1,7)])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "source_map": [
   12,
   75,
   79,
   83,
   89,
   93,
   96,
   100,
   106,
   110,
   112,
   118,
   123,
   129,
   137,
   161,
   171,
   175
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}