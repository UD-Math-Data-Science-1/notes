{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e35a6e",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "As a general term, *regularization* refers to modifying something that is difficult to always compute accurately with something more tractable. For learning models, regularization is a common way to combat overfitting.\n",
    "\n",
    "Imagine we had an $\\real^{n\\times 4}$ feature matrix in which the features are identical; that is, the predictor variables satisfy $x_1=x_2=x_3=x_4$, and suppose the target $y$ also equals $x_1$. Clearly, we get a perfect regression if we use\n",
    "\n",
    "$$\n",
    "y = 1x_1 + 0x_2 + 0x_3 + 0x_4.\n",
    "$$\n",
    "\n",
    "But an equally good regression is \n",
    "\n",
    "$$\n",
    "y = \\frac{1}{4}x_1 + \\frac{1}{4}x_2 + \\frac{1}{4}x_3 + \\frac{1}{4}x_4.\n",
    "$$\n",
    "\n",
    "For that matter, so is\n",
    "\n",
    "$$\n",
    "y = 1000x_1 - 500x_2 - 500x_3 + 1x_4.\n",
    "$$\n",
    "\n",
    "A problem with more than one solution is called **ill-posed**. If we made tiny changes to the predictor variables in this thought experiment, the problem would technically be well-posed, but there would be a wide range of solutions that were very nearly correct, in which case the problem is said to be **ill conditioned**, and for practical purposes it remains just as difficult.\n",
    "\n",
    "The ill conditioning can be regularized away by modifying the least squares loss function to penalize complexity in the model, in the form of excessively large regression coefficients. The common choices are **ridge regression**,\n",
    "\n",
    "$$\n",
    "L(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\twonorm{\\bfw}^2,\n",
    "$$\n",
    "\n",
    "and **LASSO**, \n",
    "\n",
    "$$\n",
    "L(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\onenorm{\\bfw}.\n",
    "$$\n",
    "\n",
    "As $\\alpha\\to 0$, both forms revert to the usual least squares loss, but as $\\alpha \\to \\infty$, the optimization becomes increasingly concerned with prioritizing a small result for $\\bfw$. \n",
    "\n",
    "While ridge regression is an easier function to minimize quickly, LASSO has an interesting advantage, as illustrated in this figure.\n",
    "\n",
    "```{figure} ../_static/regularization.png\n",
    "```\n",
    "\n",
    "LASSO tends to produce **sparse** results, meaning that some of the regression coefficients are zero or negligible. These zeros indicate predictor variables that have minor predictive value, which can be valuable information in itself. Moreover, when regression is run without these variables, there may be little effect on the bias, but a reduction in variance.\n",
    "\n",
    "## Case study: Diabetes progression\n",
    "\n",
    "We'll apply regularized regression to data collected about the progression of diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1eba1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  target  \n",
       "0   -0.002592  0.019908 -0.017646   151.0  \n",
       "1   -0.039493 -0.068330 -0.092204    75.0  \n",
       "2   -0.002592  0.002864 -0.025930   141.0  \n",
       "3    0.034309  0.022692 -0.009362   206.0  \n",
       "4   -0.002592 -0.031991 -0.046641   135.0  \n",
       "..        ...       ...       ...     ...  \n",
       "437 -0.002592  0.031193  0.007207   178.0  \n",
       "438  0.034309 -0.018118  0.044485   104.0  \n",
       "439 -0.011080 -0.046879  0.015491   132.0  \n",
       "440  0.026560  0.044528 -0.025930   220.0  \n",
       "441 -0.039493 -0.004220  0.003064    57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes(as_frame=True)[\"frame\"]\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be47981",
   "metadata": {},
   "source": [
    "First, we look at basic linear regression on all 10 predictive features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec54850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model score: 0.33222203269065154\n"
     ]
    }
   ],
   "source": [
    "X = diabetes.iloc[:,:-1]\n",
    "y = diabetes.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_tr,y_tr)\n",
    "print(\"linear model score:\",lm.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb58eb",
   "metadata": {},
   "source": [
    "We will find that ridge regression improves the score a bit, at least for some hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a505d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression score: 0.35667773997495866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rr = Ridge(alpha=0.5)\n",
    "rr.fit(X_tr,y_tr)\n",
    "print(\"ridge regression score:\",rr.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38c0da",
   "metadata": {},
   "source": [
    "A LASSO regression makes a smaller improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38630711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO model score: 0.3433304728856845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lass = Lasso(alpha=0.2)\n",
    "lass.fit(X_tr,y_tr)\n",
    "print(\"LASSO model score:\",lass.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cecd7",
   "metadata": {},
   "source": [
    "However, while ridge regression still uses all of the features, LASSO ignores four of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf3d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge coeffs:\n",
      "[   8.3095978  -121.63358099  388.91388623  219.55009665  -19.77469383\n",
      "  -70.82842908 -183.84763557  125.0866159   328.9173479   100.08665295]\n",
      "LASSO coeffs:\n",
      "[  -0.          -90.57212015  546.28988664  196.86417823   -0.\n",
      "  -19.04570829 -198.35369805    0.          469.75585901    0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"ridge coeffs:\")\n",
    "print(rr.coef_)\n",
    "print(\"LASSO coeffs:\")\n",
    "print(lass.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc9032",
   "metadata": {},
   "source": [
    "We can use the magnitude of the LASSO coefficients to rank the relative importance of the predictive features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd3e966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bmi', 's5', 's3', 'bp', 'sex', 's2', 's6', 's4', 's1', 'age'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.argsort(np.abs(lass.coef_))  # sort zero to largest\n",
    "idx = idx[::-1]                       # reverse\n",
    "X.columns[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4d752",
   "metadata": {},
   "source": [
    "Finally, we will use cross-validation to compare basic regression with all factors, versus using just the top 5 factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a270e54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores with all predictors:\n",
      "mean = 0.48061, std = 0.1164\n",
      "scores with top 5 predictors:\n",
      "mean = 0.48293, std = 0.0962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "\n",
    "kf = KFold(n_splits=8,shuffle=True,random_state=10)\n",
    "\n",
    "scores = cross_val_score(lm,X,y,cv=kf)\n",
    "print(\"scores with all predictors:\")\n",
    "print(f\"mean = {scores.mean():.5f}, std = {scores.std():.4f}\")\n",
    "\n",
    "scores = cross_val_score(lm,X.iloc[:,idx[:5]],y,cv=kf)\n",
    "print(\"scores with top 5 predictors:\")\n",
    "print(f\"mean = {scores.mean():.5f}, std = {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acee3cf",
   "metadata": {},
   "source": [
    "When fewer factors are used, we see some reduction in variance, and the mean testing score actually goes up a bit as well."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "source_map": [
   12,
   63,
   67,
   71,
   83,
   87,
   92,
   96,
   101,
   105,
   110,
   114,
   119,
   123,
   135
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}