{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafc9221",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "As a general term, *regularization* refers to modifying something that is difficult to compute accurately with something more tractable. For learning models, regularization is a common way to combat overfitting.\n",
    "\n",
    "Imagine we had an $\\real^{n\\times 4}$ feature matrix in which the features are identical; that is, the predictor variables satisfy $x_1=x_2=x_3=x_4$, and suppose the target $y$ also equals $x_1$. Clearly, we get a perfect regression if we use\n",
    "\n",
    "$$\n",
    "y = 1x_1 + 0x_2 + 0x_3 + 0x_4.\n",
    "$$\n",
    "\n",
    "But an equally good regression is \n",
    "\n",
    "$$\n",
    "y = \\frac{1}{4}x_1 + \\frac{1}{4}x_2 + \\frac{1}{4}x_3 + \\frac{1}{4}x_4.\n",
    "$$\n",
    "\n",
    "For that matter, so is\n",
    "\n",
    "$$\n",
    "y = 1000x_1 - 500x_2 - 500x_3 + 1x_4.\n",
    "$$\n",
    "\n",
    "A problem with more than one solution is called **ill-posed**. If we made tiny changes to the predictor variables in this thought experiment, the problem would technically be well-posed, but there would be a wide range of solutions that were very nearly correct, in which case the problem is said to be **ill conditioned**, and for practical purposes it remains just as difficult.\n",
    "\n",
    "The ill conditioning can be regularized away by modifying the least squares loss function to penalize complexity in the model, in the form of excessively large regression coefficients. The common choices are **ridge regression**,\n",
    "\n",
    "$$\n",
    "L(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\twonorm{\\bfw}^2,\n",
    "$$\n",
    "\n",
    "and **LASSO**, \n",
    "\n",
    "$$\n",
    "L(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\onenorm{\\bfw}.\n",
    "$$\n",
    "\n",
    "As $\\alpha\\to 0$, both forms revert to the usual least squares loss, but as $\\alpha \\to \\infty$, the optimization becomes increasingly concerned with prioritizing a small result for $\\bfw$. \n",
    "\n",
    "While ridge regression is an easier function to minimize quickly, LASSO has an interesting advantage, as illustrated in this figure.\n",
    "\n",
    "```{figure} ../_static/regularization.png\n",
    "```\n",
    "\n",
    "LASSO tends to produce **sparse** results, meaning that some of the regression coefficients are zero or negligible. These zeros indicate predictor variables that have minor predictive value, which can be valuable information in itself. Moreover, when regression is run without these variables, there may be little effect on the bias, but a reduction in variance.\n",
    "\n",
    "## Case study: Diabetes progression\n",
    "\n",
    "We'll apply regularized regression to data collected about the progression of diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa07f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  target  \n",
       "0   -0.002592  0.019908 -0.017646   151.0  \n",
       "1   -0.039493 -0.068330 -0.092204    75.0  \n",
       "2   -0.002592  0.002864 -0.025930   141.0  \n",
       "3    0.034309  0.022692 -0.009362   206.0  \n",
       "4   -0.002592 -0.031991 -0.046641   135.0  \n",
       "..        ...       ...       ...     ...  \n",
       "437 -0.002592  0.031193  0.007207   178.0  \n",
       "438  0.034309 -0.018118  0.044485   104.0  \n",
       "439 -0.011080 -0.046879  0.015491   132.0  \n",
       "440  0.026560  0.044528 -0.025930   220.0  \n",
       "441 -0.039493 -0.004220  0.003064    57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes(as_frame=True)[\"frame\"]\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a94554",
   "metadata": {},
   "source": [
    "First, we look at basic linear regression on all 10 predictive features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65154b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model CoD score: 0.4399387660024645\n"
     ]
    }
   ],
   "source": [
    "X = diabetes.drop(\"target\",axis=1)\n",
    "y = diabetes[\"target\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_tr,y_tr)\n",
    "print(\"linear model CoD score:\",lm.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281e7e2",
   "metadata": {},
   "source": [
    "We will find that ridge regression improves the score a bit, at least for some hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0607c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression CoD score: 0.4519973816947853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rr = Ridge(alpha=0.1)  # alpha weights the regularization term\n",
    "rr.fit(X_tr,y_tr)\n",
    "print(\"ridge regression CoD score:\",rr.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fd637",
   "metadata": {},
   "source": [
    "Ridge regularization added a penalty for the 2-norm of the regression coefficients vector. Accordingly, the regularized solution has smaller coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee70ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-norm of unregularized model coefficients: 1525.2110583591316\n",
      "2-norm of ridge regression coefficients: 808.5402825823023\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "print(\"2-norm of unregularized model coefficients:\",norm(lm.coef_))\n",
    "print(\"2-norm of ridge regression coefficients:\",norm(rr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2be164",
   "metadata": {},
   "source": [
    "As we continue to increase the regularization parameter, the method becomes increasingly obsessed with keeping the coefficient vector small, and pays ever less attention to the data as a result. Eventually, the quality of fit will decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8890d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-norm of coefficient vector: 238.894333259972\n",
      "ridge regression CoD score: 0.2659744571622481\n"
     ]
    }
   ],
   "source": [
    "rr = Ridge(alpha=4)  # more regularization\n",
    "rr.fit(X_tr,y_tr)\n",
    "print(\"2-norm of coefficient vector:\",norm(rr.coef_))\n",
    "print(\"ridge regression CoD score:\",rr.score(X_te,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907e21d",
   "metadata": {},
   "source": [
    "LASSO penalizes the 1-norm of the coefficient vector. Here's a LASSO regression fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d63b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO model CoD score: 0.4382313890360462\n",
      "1-norm of LASSO coefficient vector: 1906.369282881487\n",
      "1-norm of unregularized coefficient vector: 3722.853581168856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lass = Lasso(alpha=0.05)\n",
    "lass.fit(X_tr,y_tr)\n",
    "print(\"LASSO model CoD score:\",lass.score(X_te,y_te))\n",
    "print(\"1-norm of LASSO coefficient vector:\",norm(lass.coef_,1))\n",
    "print(\"1-norm of unregularized coefficient vector:\",norm(lm.coef_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa590d",
   "metadata": {},
   "source": [
    "A validation curve suggests modest gains in the $R^2$ score as the regularization parameter is varied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c3d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,validation_curve\n",
    "import numpy as np\n",
    "kf = KFold(n_splits=4,shuffle=True,random_state=0)\n",
    "\n",
    "alpha = np.linspace(0,0.1,80)[1:]  # exclude alpha=0\n",
    "_,scores = validation_curve(lass,X_tr,y_tr,cv=kf,param_name=\"alpha\",param_range=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55797e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/home/driscoll/Dropbox/class/267/notes/_build/jupyter_execute/regression/regularization_14_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.relplot(x=alpha,y=np.mean(scores,axis=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95230f5",
   "metadata": {},
   "source": [
    "However, while ridge regression still uses all of the features, LASSO ignores four of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc68f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge coeffs:\n",
      "[ 35.59882969  -5.65413725 128.72386461  95.80592564  26.99211072\n",
      "  13.94765865 -73.92991399  68.3705389  121.92677535  63.9783969 ]\n",
      "LASSO coeffs:\n",
      "[  -0.         -155.28275008  529.16520628  313.42663726 -132.50563763\n",
      "   -0.         -165.17345325    0.          580.25912765   30.55647073]\n"
     ]
    }
   ],
   "source": [
    "print(\"ridge coeffs:\")\n",
    "print(rr.coef_)\n",
    "lass = Lasso(alpha=0.05)\n",
    "lass.fit(X_tr,y_tr)\n",
    "print(\"LASSO coeffs:\")\n",
    "print(lass.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2baf9d5",
   "metadata": {},
   "source": [
    "We can use the magnitude of the LASSO coefficients to rank the relative importance of the predictive features. We have to make sure to take the absolute values of the coefficients, because we don't care about whether an effect is positive or negative, just its magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ac1b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 3, 6, 1, 4, 9, 7, 5, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Get the permutation that sorts values in increasing order.\n",
    "idx = np.argsort(np.abs(lass.coef_))  \n",
    "idx = idx[::-1]    # reverse the order\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e0e77",
   "metadata": {},
   "source": [
    "The last three features were dropped by LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f336bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['s4', 's2', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "zeroed = X.columns[idx[-3:]]\n",
    "print(zeroed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f1219",
   "metadata": {},
   "source": [
    "Now we can drop these features from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9ead8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s3</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.069797</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.070730</td>\n",
       "      <td>-0.062913</td>\n",
       "      <td>0.040343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.011039</td>\n",
       "      <td>-0.057314</td>\n",
       "      <td>-0.024960</td>\n",
       "      <td>0.030232</td>\n",
       "      <td>0.017037</td>\n",
       "      <td>-0.005220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.020218</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>-0.013953</td>\n",
       "      <td>0.059685</td>\n",
       "      <td>-0.096433</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.045529</td>\n",
       "      <td>0.090730</td>\n",
       "      <td>-0.018080</td>\n",
       "      <td>0.070730</td>\n",
       "      <td>-0.034524</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.054707</td>\n",
       "      <td>-0.077971</td>\n",
       "      <td>-0.033216</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>-0.019197</td>\n",
       "      <td>-0.005220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sex       bmi        bp        s1        s3        s5        s6\n",
       "70  -0.044642 -0.069797 -0.012556 -0.000193  0.070730 -0.062913  0.040343\n",
       "37  -0.044642  0.011039 -0.057314 -0.024960  0.030232  0.017037 -0.005220\n",
       "170  0.050680 -0.020218 -0.036656 -0.013953  0.059685 -0.096433 -0.017646\n",
       "400 -0.044642  0.045529  0.090730 -0.018080  0.070730 -0.034524 -0.009362\n",
       "286 -0.044642 -0.054707 -0.077971 -0.033216  0.140681 -0.019197 -0.005220"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_reduced = X_tr.drop(zeroed,axis=1)\n",
    "X_te_reduced = X_te.drop(zeroed,axis=1)\n",
    "X_tr_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a7b17",
   "metadata": {},
   "source": [
    "Returning to the original, unregularized fit, we find that hardly anything is lost by using the reduced feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d75d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original linear model score: 0.4399387660024645\n",
      "reduced linear model score: 0.4388377188754573\n"
     ]
    }
   ],
   "source": [
    "print(\"original linear model score:\",lm.score(X_te,y_te))\n",
    "lm.fit(X_tr_reduced,y_tr)\n",
    "print(\"reduced linear model score:\",lm.score(X_te_reduced,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f5c77",
   "metadata": {},
   "source": [
    "<div style=\"max-width:608px\"><div style=\"position:relative;padding-bottom:66.118421052632%\"><iframe id=\"kaltura_player\" src=\"https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_irlwjqis&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_zgo9xrkv\" width=\"608\" height=\"402\" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow=\"autoplay *; fullscreen *; encrypted-media *\" sandbox=\"allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation\" frameborder=\"0\" title=\"Kaltura Player\" style=\"position:absolute;top:0;left:0;width:100%;height:100%\"></iframe></div></div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "source_map": [
   12,
   63,
   67,
   71,
   82,
   86,
   91,
   95,
   99,
   103,
   108,
   112,
   119,
   123,
   132,
   135,
   139,
   146,
   150,
   156,
   160,
   163,
   167,
   171,
   175,
   179
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}